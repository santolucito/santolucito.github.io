{
  "paths": [
    {
      "type": "file",
      "value": "index.md"
    },
    {
      "type": "dir",
      "name": "0_Introduction",
      "children": [
        {
          "type": "file",
          "value": "0_Introduction/index.md"
        }
      ]
    },
    {
      "type": "dir",
      "name": "1_Background",
      "children": [
        {
          "type": "dir",
          "name": "1_Preliminaries",
          "children": [
            {
              "type": "file",
              "value": "1_Background/1_Preliminaries/1_Development_Tools.md"
            },
            {
              "type": "file",
              "value": "1_Background/1_Preliminaries/2_Hardware.md"
            },
            {
              "type": "file",
              "value": "1_Background/1_Preliminaries/3_Communication.md"
            },
            {
              "type": "file",
              "value": "1_Background/1_Preliminaries/index.md"
            }
          ]
        },
        {
          "type": "dir",
          "name": "2_Creativity",
          "children": [
            {
              "type": "file",
              "value": "1_Background/2_Creativity/1_Whence_creativity.md"
            },
            {
              "type": "file",
              "value": "1_Background/2_Creativity/2_Presenting_your_work.md"
            },
            {
              "type": "file",
              "value": "1_Background/2_Creativity/index.md"
            }
          ]
        }
      ]
    },
    {
      "type": "dir",
      "name": "2_Modules",
      "children": [
        {
          "type": "dir",
          "name": "1_Generative_Art",
          "children": [
            {
              "type": "file",
              "value": "2_Modules/1_Generative_Art/index.md"
            }
          ]
        },
        {
          "type": "dir",
          "name": "2_Interactive_Devices",
          "children": [
            {
              "type": "file",
              "value": "2_Modules/2_Interactive_Devices/index.md"
            }
          ]
        },
        {
          "type": "dir",
          "name": "3_Installation_Art",
          "children": [
            {
              "type": "file",
              "value": "2_Modules/3_Installation_Art/index.md"
            }
          ]
        },
        {
          "type": "dir",
          "name": "4_Kinetic_Scuplture",
          "children": [
            {
              "type": "file",
              "value": "2_Modules/4_Kinetic_Scuplture/index.md"
            }
          ]
        },
        {
          "type": "dir",
          "name": "7_Final_Project",
          "children": [
            {
              "type": "file",
              "value": "2_Modules/7_Final_Project/index.md"
            }
          ]
        },
        {
          "type": "file",
          "value": "2_Modules/index.md"
        }
      ]
    },
    {
      "type": "dir",
      "name": "Appendix",
      "children": [
        {
          "type": "file",
          "value": "Appendix/How_Low_Can_you_Go.md"
        },
        {
          "type": "file",
          "value": "Appendix/Run_on_boot.md"
        },
        {
          "type": "file",
          "value": "Appendix/System_Management.md"
        },
        {
          "type": "file",
          "value": "Appendix/Version_Control.md"
        },
        {
          "type": "file",
          "value": "Appendix/index.md"
        }
      ]
    },
    {
      "type": "dir",
      "name": "ExternalResources",
      "children": [
        {
          "type": "dir",
          "name": "Code",
          "children": [
            {
              "type": "file",
              "value": "ExternalResources/Code/getESP32.sh"
            }
          ]
        },
        {
          "type": "dir",
          "name": "Documentation",
          "children": [
            {
              "type": "file",
              "value": "ExternalResources/Documentation/Esp32_DevKit_pinmap.png"
            },
            {
              "type": "file",
              "value": "ExternalResources/Documentation/esp32_datasheet_en.pdf"
            }
          ]
        },
        {
          "type": "dir",
          "name": "Media",
          "children": [
            {
              "type": "file",
              "value": "ExternalResources/Media/ceid_cafe.jpg"
            },
            {
              "type": "file",
              "value": "ExternalResources/Media/columbia_video.jpg"
            }
          ]
        },
        {
          "type": "dir",
          "name": "Readings",
          "children": [
            {
              "type": "file",
              "value": "ExternalResources/Readings/Beginners_Guide_v1.pdf"
            },
            {
              "type": "file",
              "value": "ExternalResources/Readings/Live-Coding-Education.pdf"
            },
            {
              "type": "file",
              "value": "ExternalResources/Readings/MagPi68.pdf"
            }
          ]
        },
        {
          "type": "file",
          "value": "ExternalResources/index.md"
        }
      ]
    }
  ],
  "contents": [
    {
      "path": "index.md",
      "url": "index.html",
      "content": "# Creative Embedded Systems\n\nThis website holds the materials for Creative Embedded Systems.\nA version of this course was taught at Yale, Fall 2020 as CPSC334.\nA second version of this course will be taught at Barnard/Columbia, Spring 2021 as COMS 3930.\n\nThe syllabus for COMS 3930 is [available here](./syllabus.pdf).\n\nOffice hours Mondays 3-4pm EST and Fridays 1-2pm EST. Link available on Courseworks.\n\nWhile most of these materials are generalizable outside the context of Yale abd Barnard/Columbia, some material is specific to the resources available on campus.\n\nThe course text is co-written by Scott Petersen and Mark Santolucito.\nThis is an evolving text, so expect changes.\n\nYou can navigate the materials with the bar to the left.\nPlease let us know about typos or other errata via email.\n\n",
      "html": "<h1>Creative Embedded Systems</h1>\n<p>This website holds the materials for Creative Embedded Systems.\nA version of this course was taught at Yale, Fall 2020 as CPSC334.\nA second version of this course will be taught at Barnard/Columbia, Spring 2021 as COMS 3930.</p>\n<p>The syllabus for COMS 3930 is <a href=\"./syllabus.pdf\">available here</a>.</p>\n<p>Office hours Mondays 3-4pm EST and Fridays 1-2pm EST. Link available on Courseworks.</p>\n<p>While most of these materials are generalizable outside the context of Yale abd Barnard/Columbia, some material is specific to the resources available on campus.</p>\n<p>The course text is co-written by Scott Petersen and Mark Santolucito.\nThis is an evolving text, so expect changes.</p>\n<p>You can navigate the materials with the bar to the left.\nPlease let us know about typos or other errata via email.</p>\n",
      "id": 0
    },
    {
      "path": "0_Introduction/index.md",
      "url": "0_Introduction/index.html",
      "content": "# Introduction\n\nUbiquitous computing is creating new canvases and opportunities for creative ideas.\nThis text, and the course for which it was written, is designed as a resource for those interested in applying embedded computation to their art.\nWhile this text will be primarily of interest to artists who wish to expand the scope of their practice, it also may serve as an introductory text to embedded systems for more traditional computer science and engineering students.\n\nThis text is laid out according to the following structure:\n\n## Chapter 0: Introduction\n\nThis first chapter serves to provide a guide for the rest of the manuscript.\nWe provide an overview of the goals of the text, how this text can be used in a classroom setting, and a dependency chart to help readers select the most relevant chapters to their own work.\n\n## Section 1: Background\n\nThe background section contains two parts. First, [Preliminaries](../1_Background/1_Preliminaries/index.md), which is intended to bring the reader up to speed with a high level overview current embedded technologies. \nSecond, [Creativity](../1_Background/2_Creativity/index.md), which is intended to give the reader a theoretical grounding in understanding what creativity means in the context of embedded systems.\n\nThe [Preliminaries](../1_Background/1_Preliminaries/index.md) chapter gives readers context on current trends in embedded systems as they are applied to artistic creation.\nIt is highly likely that over time this chapter will become less relevant with the introduction of new technologies.\nWith this in mind, the reader should expect the material in the Preliminaries to lose relevance over a span of 5-15 years.\nIf the reader is familiar with current trends in embedded systems, this chapter may be used instead as a reference.\nWe will refer back to this chapter throughout the text to provide concrete examples of how creative designs may be implemented.\n\nThe [Creativity](../1_Background/2_Creativity/index.md) chapter lays the framework to understand what it means to make a creative choice in designing an artistic embedded system.\n\n## Section 2: Modules\n\nThis section forms the core of this text and introduces, in a principled way, various designs of embedded systems.\nWe introduce the concept of a 'system configuration' to guide the exploration of creative applications of embedded systems.\nEach module follows the same overall structure:\n\n1. System configuration\n\nEach module starts with a description of the system configuration we will consider in the given module.\nIn general, we give a visual graph representation of the system configuration, falling back to the formal textual description of the system only when clarification is needed.\n\n2. Example Application domains\n\nAfter introducing the system configuration of the module, we explore some application domains of the configuration.\nBy demonstrating application domains, we address through examples some of the common creative choices to be made, as well as some technical issues that often arise in this configuration.\nConcrete examples of work that artists have presented in this domain gives a better intuition for the type of systems that can be captured by the given system configuration.\nThese examples serve two purposes - first, they should help inspire the reader's own work and second, they provide a entry point to building a new creative embedded system.\nThe reader may attempt to recreate the given example and in this process is likely to encounter technical issues that will help build expertise.\n \n3. Assignments\n\nFinally, we provide an assignment for this module.\nAssignments will often fall into the same domain as the example application domains we have presented.\nIn general, assignments leave most of the criteria undefined, but this is by design.\nOne of the primary challenges in working the artistic technology space is handling a lack a specifications.\nThe overarching goal of these assignments is to push the reader to become more comfortable working on concrete tasks (ie programming) in an inherently amorphous space (ie art).\n\n## Appendix\n\nThe appendix provides detailed discussions of development and design issues likely to arise in the development of creative embedded systems.\nEach chapter of the appendix covers a technical issue that is highly dependent of the systems used in this text, as described in [Preliminaries](../1_Background/1_Preliminaries/index.md).\nWith this in mind, the reader should expect the material in the appendix to lose relevance over a span of 3-5 years.\n",
      "html": "<h1>Introduction</h1>\n<p>Ubiquitous computing is creating new canvases and opportunities for creative ideas.\nThis text, and the course for which it was written, is designed as a resource for those interested in applying embedded computation to their art.\nWhile this text will be primarily of interest to artists who wish to expand the scope of their practice, it also may serve as an introductory text to embedded systems for more traditional computer science and engineering students.</p>\n<p>This text is laid out according to the following structure:</p>\n<h2>Chapter 0: Introduction</h2>\n<p>This first chapter serves to provide a guide for the rest of the manuscript.\nWe provide an overview of the goals of the text, how this text can be used in a classroom setting, and a dependency chart to help readers select the most relevant chapters to their own work.</p>\n<h2>Section 1: Background</h2>\n<p>The background section contains two parts. First, <a href=\"../1_Background/1_Preliminaries/index.html\">Preliminaries</a>, which is intended to bring the reader up to speed with a high level overview current embedded technologies.\nSecond, <a href=\"../1_Background/2_Creativity/index.html\">Creativity</a>, which is intended to give the reader a theoretical grounding in understanding what creativity means in the context of embedded systems.</p>\n<p>The <a href=\"../1_Background/1_Preliminaries/index.html\">Preliminaries</a> chapter gives readers context on current trends in embedded systems as they are applied to artistic creation.\nIt is highly likely that over time this chapter will become less relevant with the introduction of new technologies.\nWith this in mind, the reader should expect the material in the Preliminaries to lose relevance over a span of 5-15 years.\nIf the reader is familiar with current trends in embedded systems, this chapter may be used instead as a reference.\nWe will refer back to this chapter throughout the text to provide concrete examples of how creative designs may be implemented.</p>\n<p>The <a href=\"../1_Background/2_Creativity/index.html\">Creativity</a> chapter lays the framework to understand what it means to make a creative choice in designing an artistic embedded system.</p>\n<h2>Section 2: Modules</h2>\n<p>This section forms the core of this text and introduces, in a principled way, various designs of embedded systems.\nWe introduce the concept of a ‘system configuration’ to guide the exploration of creative applications of embedded systems.\nEach module follows the same overall structure:</p>\n<ol>\n<li>System configuration</li>\n</ol>\n<p>Each module starts with a description of the system configuration we will consider in the given module.\nIn general, we give a visual graph representation of the system configuration, falling back to the formal textual description of the system only when clarification is needed.</p>\n<ol start=\"2\">\n<li>Example Application domains</li>\n</ol>\n<p>After introducing the system configuration of the module, we explore some application domains of the configuration.\nBy demonstrating application domains, we address through examples some of the common creative choices to be made, as well as some technical issues that often arise in this configuration.\nConcrete examples of work that artists have presented in this domain gives a better intuition for the type of systems that can be captured by the given system configuration.\nThese examples serve two purposes - first, they should help inspire the reader’s own work and second, they provide a entry point to building a new creative embedded system.\nThe reader may attempt to recreate the given example and in this process is likely to encounter technical issues that will help build expertise.</p>\n<ol start=\"3\">\n<li>Assignments</li>\n</ol>\n<p>Finally, we provide an assignment for this module.\nAssignments will often fall into the same domain as the example application domains we have presented.\nIn general, assignments leave most of the criteria undefined, but this is by design.\nOne of the primary challenges in working the artistic technology space is handling a lack a specifications.\nThe overarching goal of these assignments is to push the reader to become more comfortable working on concrete tasks (ie programming) in an inherently amorphous space (ie art).</p>\n<h2>Appendix</h2>\n<p>The appendix provides detailed discussions of development and design issues likely to arise in the development of creative embedded systems.\nEach chapter of the appendix covers a technical issue that is highly dependent of the systems used in this text, as described in <a href=\"../1_Background/1_Preliminaries/index.html\">Preliminaries</a>.\nWith this in mind, the reader should expect the material in the appendix to lose relevance over a span of 3-5 years.</p>\n",
      "id": 1
    },
    {
      "path": "1_Background/1_Preliminaries/1_Development_Tools.md",
      "url": "1_Background/1_Preliminaries/1_Development_Tools.html",
      "content": "# Dev vs Prod\n\nWhen developing systems that require longterm reliability (e.g. installation art) and must be used by non-technical users (e.g. potentially, museum staff), it is important that you separate concerns between *developing* a system and *deploying* the system.\nIn traditional computer engineering spheres, this is referred to generally, as Dev (for development) and Prod (for production).\nFor example, a website with a database, might have a Dev database, that allows programmers to try out new ideas without impacting users, and a Prod database, that isolates user data from unstable code.\n\nSimilarly, when working on creative embedded systems, you will want to always have a working version to fall back on for those surprise demo opportunities.\nWhen someone walks by and asks what you are working on, it is much more satisfying to be able to demo something rather than give a vague description and show them a bundle of wires that doesn't do anything.\nFor this, imagine Prod as checking-pointing your progress every time you are sure you have some minimal working system.\nDev on the other hand is just a place to save changes as you go.\n\n# Version Control\n\nVersion control is a class of systems that give you the ability to track and manage changes to a code base over time.\nYou will implement the concept of Dev vs Prod through version control.\nThe tool `git` is the most popular tool at the moment, with Github being the most prominent server provider.\nAlternatives tools as such as `svn`, `cvs`, and `mercurial`, have their own benefits, but are mostly used to annoy your collaborators by forcing them to learn another version control system.\n\nThere are a number of good tutorials to using git online.\nThe most important advice that any of tutorials will give is ***commit more often***.\nIf you do not regularly commit (and push) your work, you will eventually lose code.\nIf you think this does not apply to you because you are careful with your hardware, you just haven't been programming long enough.\nCommitting your changes is especially important in the context of embedded systems development, as you will need to reach into some low-level parts of your system and may accidentally bring your system into a state where the best option is to reinstall the OS.\nFurthermore, more frequent commits, means you can write shorter commit messages that capture all the changes, which in turn means you will have an easier time remembering where you left off when you return to a project.\n\nAs a note, we will be using git to track and evaluate your work in this class.\nThis will also allow the instructors to track contributions to group projects.\nMore generally, your Github profile can be an easy way to build a portfolio of your coding projects.\n\n# Software Dev Environment\n\nThe development environment you choose is important part of working with embedded systems.\nGenerally speaking, using a Linux system will provide the smoothest work flow.\nThat being said, it is certainly possible to develop embedded systems using a Mac, Windows, or even ChromeOs.\nWe will highlight some of the steps needed to step up a working development environment for each platform\nLater chapters will assume you are working on a Linux machine, but the general approach should be quite similar across platforms.\n\n## Linux\n\nThere are many different flavors of Linux operating systems.\nThe stock Ubuntu LTS will usually be sufficient for the purposes of this class.\nIf you are planning to work in any development focused job, a working familiarity with Linux is an absolute prerequisite.\nLinux has very few safe guards in place to protect you from yourself relative to other OSes (e.g. rm -r ~/*).\nIf you enjoy mucking around in system configurations, there is also a good chance you will eventually want to wipe your system and reinstall the OS.\nThis is another reason to commit your code more often to version control.\n\n## Mac\n\nMac is a UNIX based system, which means it shares many properties with development on a Linux machines.\nIn particular, it provides a bash shell that will allow to experience development in a very similar style to Linux.\nYou will likely need to disable a number of default security settings that disallow typical users of a Mac from wrecking too much havoc in their system.\n\n## Windows\n\nWindows has traditionally lagged behind other OSes in terms of flexibility for developers.\nThis is primarily because of the difficulty of using the terminal (cmd, PowerShell, and MingW are all attempts, largely failed, to overcome this).\n\nThe situation has changed significantly with Windows Subsystems Linux (WSL).\nWSL gives you access to a Linux system running as a sort of light-weight virtual machine (VM).\nWSL will be enough for most cases of what we cover in this class.\n\nYou might also install a virtual machine (for our purposes, a way to have an OS within an OS), but this will introduce a number of issues with connecting peripheral devices and may not be the best choice for the development of embedded systems at the level we cover in this course.\n\n## ChromeOs\n\nChromeOS is essentially a wrapper on top of a Linux system.\nChromsOS has some native support for accessing the Linux system directly, but Crouton is better supported at the moment.\nCrouton allows you to access the Linux subsystem and run your favorite  distribution of Linux, either via the terminal, or with the GUI.\n\nYou may run into some performance issues if your chromebook is lacking in computational power.\nOn the other hand, this can sometimes be helpful as you will always be the first to experience and bottlenecks introduced in the code.\n\n## Cross platform support\n\nSome system configurations will require multiplatform support.\nOne example is a crowd sourced performance system, which allows the audience to interact with the performance via their device.\nIn this case, as the artists, you may want the users to download an app on their phone, or access the interface via the web.\nRather than manually building each frontend, a build system with cross platform support will allow you to automatically generate a frontend for all these systems.\nCross platform support is a large topic, far beyond the scope of this text, however we provide here a few key tips when handling cross platform development and deployments.\n\nIf you are collaborating on a project with other developers who are using a different system then your own, you must ensure _cross platform development_ support.\nThat is to say, each developer must be able to run and edit the code on their own machine.\nOne way to ensure that your changes to a project (which work on your local machine) still work on other architectures is to use Continuous Intergration (CI).\nCI is a development tool that allows you to set up a version controlled repository to rebuild and test your project on a variety of system architectures on every commit.\nOne of the most popular tools for this TravisCI, which has free integration with public repositories on GitHub.\n\n## Cross compilation\n\nIn a similar vein to cross platform support, cross compilation allows you to compile code on one machine that targets another.\nFor example, the Raspberry Pi runs on the ARM architecture, but most desktop machines at this point run x86.\nThis means a binary compiled on your desktop cannot be ported directly to the Raspberry Pi.\nGenerally, there are better solutions than cross compilation (e.g. copy the code to you Raspberry Pi and build on the device itself).\n\n\n# Hardware Dev Environment\n\nIn addition to having a dedicated setup for developing software, you may also want to have a dedicated space and workflow established for hardware development.\nMinimally, this means having a clean, flat surface to work.\n\nThis is especially important when this class is taught in a remote setting.\n",
      "html": "<h1>Dev vs Prod</h1>\n<p>When developing systems that require longterm reliability (e.g. installation art) and must be used by non-technical users (e.g. potentially, museum staff), it is important that you separate concerns between <em>developing</em> a system and <em>deploying</em> the system.\nIn traditional computer engineering spheres, this is referred to generally, as Dev (for development) and Prod (for production).\nFor example, a website with a database, might have a Dev database, that allows programmers to try out new ideas without impacting users, and a Prod database, that isolates user data from unstable code.</p>\n<p>Similarly, when working on creative embedded systems, you will want to always have a working version to fall back on for those surprise demo opportunities.\nWhen someone walks by and asks what you are working on, it is much more satisfying to be able to demo something rather than give a vague description and show them a bundle of wires that doesn’t do anything.\nFor this, imagine Prod as checking-pointing your progress every time you are sure you have some minimal working system.\nDev on the other hand is just a place to save changes as you go.</p>\n<h1>Version Control</h1>\n<p>Version control is a class of systems that give you the ability to track and manage changes to a code base over time.\nYou will implement the concept of Dev vs Prod through version control.\nThe tool <code>git</code> is the most popular tool at the moment, with Github being the most prominent server provider.\nAlternatives tools as such as <code>svn</code>, <code>cvs</code>, and <code>mercurial</code>, have their own benefits, but are mostly used to annoy your collaborators by forcing them to learn another version control system.</p>\n<p>There are a number of good tutorials to using git online.\nThe most important advice that any of tutorials will give is <em><strong>commit more often</strong></em>.\nIf you do not regularly commit (and push) your work, you will eventually lose code.\nIf you think this does not apply to you because you are careful with your hardware, you just haven’t been programming long enough.\nCommitting your changes is especially important in the context of embedded systems development, as you will need to reach into some low-level parts of your system and may accidentally bring your system into a state where the best option is to reinstall the OS.\nFurthermore, more frequent commits, means you can write shorter commit messages that capture all the changes, which in turn means you will have an easier time remembering where you left off when you return to a project.</p>\n<p>As a note, we will be using git to track and evaluate your work in this class.\nThis will also allow the instructors to track contributions to group projects.\nMore generally, your Github profile can be an easy way to build a portfolio of your coding projects.</p>\n<h1>Software Dev Environment</h1>\n<p>The development environment you choose is important part of working with embedded systems.\nGenerally speaking, using a Linux system will provide the smoothest work flow.\nThat being said, it is certainly possible to develop embedded systems using a Mac, Windows, or even ChromeOs.\nWe will highlight some of the steps needed to step up a working development environment for each platform\nLater chapters will assume you are working on a Linux machine, but the general approach should be quite similar across platforms.</p>\n<h2>Linux</h2>\n<p>There are many different flavors of Linux operating systems.\nThe stock Ubuntu LTS will usually be sufficient for the purposes of this class.\nIf you are planning to work in any development focused job, a working familiarity with Linux is an absolute prerequisite.\nLinux has very few safe guards in place to protect you from yourself relative to other OSes (e.g. rm -r ~/*).\nIf you enjoy mucking around in system configurations, there is also a good chance you will eventually want to wipe your system and reinstall the OS.\nThis is another reason to commit your code more often to version control.</p>\n<h2>Mac</h2>\n<p>Mac is a UNIX based system, which means it shares many properties with development on a Linux machines.\nIn particular, it provides a bash shell that will allow to experience development in a very similar style to Linux.\nYou will likely need to disable a number of default security settings that disallow typical users of a Mac from wrecking too much havoc in their system.</p>\n<h2>Windows</h2>\n<p>Windows has traditionally lagged behind other OSes in terms of flexibility for developers.\nThis is primarily because of the difficulty of using the terminal (cmd, PowerShell, and MingW are all attempts, largely failed, to overcome this).</p>\n<p>The situation has changed significantly with Windows Subsystems Linux (WSL).\nWSL gives you access to a Linux system running as a sort of light-weight virtual machine (VM).\nWSL will be enough for most cases of what we cover in this class.</p>\n<p>You might also install a virtual machine (for our purposes, a way to have an OS within an OS), but this will introduce a number of issues with connecting peripheral devices and may not be the best choice for the development of embedded systems at the level we cover in this course.</p>\n<h2>ChromeOs</h2>\n<p>ChromeOS is essentially a wrapper on top of a Linux system.\nChromsOS has some native support for accessing the Linux system directly, but Crouton is better supported at the moment.\nCrouton allows you to access the Linux subsystem and run your favorite  distribution of Linux, either via the terminal, or with the GUI.</p>\n<p>You may run into some performance issues if your chromebook is lacking in computational power.\nOn the other hand, this can sometimes be helpful as you will always be the first to experience and bottlenecks introduced in the code.</p>\n<h2>Cross platform support</h2>\n<p>Some system configurations will require multiplatform support.\nOne example is a crowd sourced performance system, which allows the audience to interact with the performance via their device.\nIn this case, as the artists, you may want the users to download an app on their phone, or access the interface via the web.\nRather than manually building each frontend, a build system with cross platform support will allow you to automatically generate a frontend for all these systems.\nCross platform support is a large topic, far beyond the scope of this text, however we provide here a few key tips when handling cross platform development and deployments.</p>\n<p>If you are collaborating on a project with other developers who are using a different system then your own, you must ensure <em>cross platform development</em> support.\nThat is to say, each developer must be able to run and edit the code on their own machine.\nOne way to ensure that your changes to a project (which work on your local machine) still work on other architectures is to use Continuous Intergration (CI).\nCI is a development tool that allows you to set up a version controlled repository to rebuild and test your project on a variety of system architectures on every commit.\nOne of the most popular tools for this TravisCI, which has free integration with public repositories on GitHub.</p>\n<h2>Cross compilation</h2>\n<p>In a similar vein to cross platform support, cross compilation allows you to compile code on one machine that targets another.\nFor example, the Raspberry Pi runs on the ARM architecture, but most desktop machines at this point run x86.\nThis means a binary compiled on your desktop cannot be ported directly to the Raspberry Pi.\nGenerally, there are better solutions than cross compilation (e.g. copy the code to you Raspberry Pi and build on the device itself).</p>\n<h1>Hardware Dev Environment</h1>\n<p>In addition to having a dedicated setup for developing software, you may also want to have a dedicated space and workflow established for hardware development.\nMinimally, this means having a clean, flat surface to work.</p>\n<p>This is especially important when this class is taught in a remote setting.</p>\n",
      "id": 2
    },
    {
      "path": "1_Background/1_Preliminaries/2_Hardware.md",
      "url": "1_Background/1_Preliminaries/2_Hardware.html",
      "content": "# Hardware\n\nWe will cover different types of hardware in this course:\n\n1. Microprocessors\n2. Microcontrollers\n3. Circuits\n\n## Microprocessors\n\n\nA microprocessor is a small device that can run a full OS and interface with multiple peripheral devices..\nThe main benefit to this is that you are working with, essentially, a full computer on a very small device.\nThe main challenges with using a microprocessor are power consumption, and speed (both runtime and boot time).\n\nOne of the original motivation behind microprocessors at the early stages of their more widespread adoption was to deliver more affordable access to computing power.\nHowever, With the precipitous drop in price of smartphones this is less of a concern.\nThat being said, while a smartphone can deliver the computational power for many projects, their hardware interface is severely limited.\n\n### Raspberry Pi\n\nThe Raspberry Pi (occasionally, RaspPi) is the canonical microprocessor that in large part facilitated the wide reach of the *maker movement*.\nThe Raspberry Pi is essentially a very small laptop without any peripherals (e.g. screen, keyboard, mouse), but with many open hardware ports.\nThis allows the device to easily connect to many unique or custom devices to act as sensors or actuators.\n\nThe Raspberry Pi also includes a set of GPIO (general-purpose input/output) pins, which allow you to connect to some hardware crcuits, such as buttons (inputs0 and LEDs (outputs).\nThese GPIO pins are strictly digital, which can be limiting for certain applications.\nWe will see more on this as we explore the role of microcontrollers in embedded systems.\n\nThe operating system, and all data, lives on an SD card in Raspberry Pi.\nWhen working with multiple projects that need differently configured systems, this can be leveraged to keep a separate SD card for each project.\n\nFor more detailed information on your Raspberry Pi, see the documentation available in the official (Raspeberry Pi Beginner's Guide)[].\n\nIn addition to making the Raspberry Pi devices, the Raspberry Pi Foundation, the group also produces two useful text resources.\nA magazine, MagPi, on the latest developments in the Raspberry Pi community.\nAdditionally, the Foundation annually produces the *Official Project Books*, which document a number of interesting projects that have used a Raspberry Pi.\n\n### Smartphones (Honorable mention)\n\nAlthough limited in their ability to interface with peripheral devices, a smartphone is still able to communicate with networked devices.\nA phone can be used as interface (e.g. light sensors, cameras, gyroscope, microphone) or as a portable computing device.\nIn the case of the use of the phone as a portable computing device, computation may happen locally, or the phone can simply be used as a conduit for server-side computation.\nThe difficulty with a phone as local computing device is the code is general specialized to the platform.\nWe may see this change with the introduction of Purism's Librem 5 Linux phone.\n\n\n## Microcontrollers\n\nMicrocontrollers have no OS installed (generally), which means any code you want to run on a microcontroller must be written with the specific device in mind.\nWhen designing a embedded system, you will need to consider if you application is more appropriate for a microprocessor (with GPIO pins, like the Raspberry Pi), or a microcontroller, or some combination of both.\nSome factors to consider are power draw (significantly higher for microprocessors), processing speed (significantly higher for microprocessors), and circuitry availability (generally, microcontrollers will expose more interfaces than microprocessors).\n\nIn terms of circuitry, two of the most important pieces you will find available on most microcontrollers are an ADC (Analog to Digital Converter) and a DAC (Digital to Analog Converter).\nYou will often encounter these devices in the audio domain (where we need to converter digital music signals into audio we can here - or to capture audio waves in the air into a digital signal we can record), but also in any other *continuous* domain.\nSome examples of this include light sensors and (some) motor actuators.\n\n\n### Arduino\n\nArduino is the most widely recognized type of microcontroller.\nThe Arduino, like the Raspberry Pi, was at the forefront of the maker movement.\nThe Arduino Genuino, the classic Arduino model, includes both analog and digital input pins.\nOne of the major benefits introduced by the Arduino is the Arduino programming language and the Arduino IDE.\nPreviously, the toolchain to flash code onto a microcontroller was fairly involved. \nThe Arduino IDE simplified this process.\n\nThere are two main drawbacks to the Arduino IDE - performance and space.\nThe libraries for the Arduino are not too large, but program space can be fairly limited on microcontrollers.\nAccordingly, loading these libraries at boot-time also introduces a delay.\nDepending on your application, you may find these drawbacks to be unacceptable.\nIn the creative embedded systems domain, this will rarely be the case.\n\nIt is slightly more likely that you will find the performance of the program itself to be too slow.\nThis will most often happen in the domain of interface design, where realtime interaction is key.\nDepending on the application, the human psychology can find delays of > 20 ms to be unnatural.\nGenerally, the Arduino code will not be the bottleneck for this delay.\nFor creative embedded systems, it is generally good practice to initially prototype with the Arduino IDE, and only switch to a lower level language if needed.\nMore discussion on this topic is contained in the [appendix](../../Appendix/How_Low_Can_you_Go.md).\n\nSince its introduction, there have been a number of similar microcontrollers that mimic the Arduino and can also interface with the Arduino IDE.\n\n### ESP32/ESP8266\n\nThis class of microcontrollers are essentially Arduino's with WiFi shields baked in (and so much more).\nThese devices can also be programmed from within the Arduino IDE.\nThey are likely the most popular microcontroller for creative embedded projects at the moment.\n\nThis course will focus on development with ESP32s.\n\n### MSP430 (Honorable mention)\n\nThe MSP430 is an energy harvesting device that is able to run off of the energy from ambient radio waves or any other harvestable power source.\nThis is particularly of interest in the domain of creative embedded systems as it lets us permanently untether from power.\nWhile an intriguing concept, for most creative applications, a battery is enough to untether from power.\n\nWhile the MSP430 has an fork of Arduino IDE that can be used to program the MSP430, the overhead is generally too high for the performance sensitive device.\n\n### FPGA (Honorable mention)\n\nField programmable gate arrays (FPGAs) are an exciting way to merge the speed of circuits with the ease of development of software.\nAlthough we will not cover FPGAs in depth, it is good to be aware of the sorts of problems that can be addressed with this tool.\nShort of designing a hardware circuit by hand from components (transistors, etc) and getting it printed onto a PCB (printed circuit board), FPGAs are the fastest tool you can get.\nThis can be useful for applications where realtime interaction (especially reaction) is important.\nOne example would be a computer vision module that moves a trash can to catch a ball of paper no matter where you throw it in the room.\n\n\n## Circuits\n\nAt times, we will need to build custom interfaces to the physical world.\nThis will require us to work with copper wire, resistors, sensors, and the like.\nWorking with circuits affords a huge amount of expressive freedom in terms of form factor and function, but prototyping can be a bit slow.\nThere are a number of online tools for circuit simulation that will allow you to build a simulated circuit with software, and verify that you are getting the behavior you expect.\nAn alternative development approach, and more in the spirit of the exploratory ethos of the creative embedded systems community, is to move fast and break things. \nFor the majority of components available in your average maker space, the total cost a circuit is not more than a few cents.\nThese days, it is okay if you burn a few LEDs while learning how resistors work.\nJust be sure to know when you are dealing with more expensive components, such as motor actuators or more exotic sensors.\n\n### PCB (Honorable mention)\n\nA Printed Circuit Board (PBC) is just a more stable version of a breadboard.\nPrinting a PCB is a nice way to add polish to your system, but is more of a late-stage prototyping step.\nPCBs are first designed in software, then physically printed - usually by sending the design to a PCB manufacturer.\nSome maker spaces will have PCB mills allowing you to more quickly prototype with PCB.\n\nOne difficulty to watch out for is the difference between \"through hole\" and SMT (surface mount technology components).\nTo start, you will want to stick with \"through hole\" components as they are easier to purchase and easier to use.\n\nYou may be tempted to design a PCB with the raw processor that is used on your dev board.\nWhile this can shrink the form factor, you should be sure that the smaller form factor is necessary, use the processor instead of the dev board introduced a high engineering overhead.",
      "html": "<h1>Hardware</h1>\n<p>We will cover different types of hardware in this course:</p>\n<ol>\n<li>Microprocessors</li>\n<li>Microcontrollers</li>\n<li>Circuits</li>\n</ol>\n<h2>Microprocessors</h2>\n<p>A microprocessor is a small device that can run a full OS and interface with multiple peripheral devices…\nThe main benefit to this is that you are working with, essentially, a full computer on a very small device.\nThe main challenges with using a microprocessor are power consumption, and speed (both runtime and boot time).</p>\n<p>One of the original motivation behind microprocessors at the early stages of their more widespread adoption was to deliver more affordable access to computing power.\nHowever, With the precipitous drop in price of smartphones this is less of a concern.\nThat being said, while a smartphone can deliver the computational power for many projects, their hardware interface is severely limited.</p>\n<h3>Raspberry Pi</h3>\n<p>The Raspberry Pi (occasionally, RaspPi) is the canonical microprocessor that in large part facilitated the wide reach of the <em>maker movement</em>.\nThe Raspberry Pi is essentially a very small laptop without any peripherals (e.g. screen, keyboard, mouse), but with many open hardware ports.\nThis allows the device to easily connect to many unique or custom devices to act as sensors or actuators.</p>\n<p>The Raspberry Pi also includes a set of GPIO (general-purpose input/output) pins, which allow you to connect to some hardware crcuits, such as buttons (inputs0 and LEDs (outputs).\nThese GPIO pins are strictly digital, which can be limiting for certain applications.\nWe will see more on this as we explore the role of microcontrollers in embedded systems.</p>\n<p>The operating system, and all data, lives on an SD card in Raspberry Pi.\nWhen working with multiple projects that need differently configured systems, this can be leveraged to keep a separate SD card for each project.</p>\n<p>For more detailed information on your Raspberry Pi, see the documentation available in the official (Raspeberry Pi Beginner’s Guide)[].</p>\n<p>In addition to making the Raspberry Pi devices, the Raspberry Pi Foundation, the group also produces two useful text resources.\nA magazine, MagPi, on the latest developments in the Raspberry Pi community.\nAdditionally, the Foundation annually produces the <em>Official Project Books</em>, which document a number of interesting projects that have used a Raspberry Pi.</p>\n<h3>Smartphones (Honorable mention)</h3>\n<p>Although limited in their ability to interface with peripheral devices, a smartphone is still able to communicate with networked devices.\nA phone can be used as interface (e.g. light sensors, cameras, gyroscope, microphone) or as a portable computing device.\nIn the case of the use of the phone as a portable computing device, computation may happen locally, or the phone can simply be used as a conduit for server-side computation.\nThe difficulty with a phone as local computing device is the code is general specialized to the platform.\nWe may see this change with the introduction of Purism’s Librem 5 Linux phone.</p>\n<h2>Microcontrollers</h2>\n<p>Microcontrollers have no OS installed (generally), which means any code you want to run on a microcontroller must be written with the specific device in mind.\nWhen designing a embedded system, you will need to consider if you application is more appropriate for a microprocessor (with GPIO pins, like the Raspberry Pi), or a microcontroller, or some combination of both.\nSome factors to consider are power draw (significantly higher for microprocessors), processing speed (significantly higher for microprocessors), and circuitry availability (generally, microcontrollers will expose more interfaces than microprocessors).</p>\n<p>In terms of circuitry, two of the most important pieces you will find available on most microcontrollers are an ADC (Analog to Digital Converter) and a DAC (Digital to Analog Converter).\nYou will often encounter these devices in the audio domain (where we need to converter digital music signals into audio we can here - or to capture audio waves in the air into a digital signal we can record), but also in any other <em>continuous</em> domain.\nSome examples of this include light sensors and (some) motor actuators.</p>\n<h3>Arduino</h3>\n<p>Arduino is the most widely recognized type of microcontroller.\nThe Arduino, like the Raspberry Pi, was at the forefront of the maker movement.\nThe Arduino Genuino, the classic Arduino model, includes both analog and digital input pins.\nOne of the major benefits introduced by the Arduino is the Arduino programming language and the Arduino IDE.\nPreviously, the toolchain to flash code onto a microcontroller was fairly involved.\nThe Arduino IDE simplified this process.</p>\n<p>There are two main drawbacks to the Arduino IDE - performance and space.\nThe libraries for the Arduino are not too large, but program space can be fairly limited on microcontrollers.\nAccordingly, loading these libraries at boot-time also introduces a delay.\nDepending on your application, you may find these drawbacks to be unacceptable.\nIn the creative embedded systems domain, this will rarely be the case.</p>\n<p>It is slightly more likely that you will find the performance of the program itself to be too slow.\nThis will most often happen in the domain of interface design, where realtime interaction is key.\nDepending on the application, the human psychology can find delays of &gt; 20 ms to be unnatural.\nGenerally, the Arduino code will not be the bottleneck for this delay.\nFor creative embedded systems, it is generally good practice to initially prototype with the Arduino IDE, and only switch to a lower level language if needed.\nMore discussion on this topic is contained in the <a href=\"../../Appendix/How_Low_Can_you_Go.html\">appendix</a>.</p>\n<p>Since its introduction, there have been a number of similar microcontrollers that mimic the Arduino and can also interface with the Arduino IDE.</p>\n<h3>ESP32/ESP8266</h3>\n<p>This class of microcontrollers are essentially Arduino’s with WiFi shields baked in (and so much more).\nThese devices can also be programmed from within the Arduino IDE.\nThey are likely the most popular microcontroller for creative embedded projects at the moment.</p>\n<p>This course will focus on development with ESP32s.</p>\n<h3>MSP430 (Honorable mention)</h3>\n<p>The MSP430 is an energy harvesting device that is able to run off of the energy from ambient radio waves or any other harvestable power source.\nThis is particularly of interest in the domain of creative embedded systems as it lets us permanently untether from power.\nWhile an intriguing concept, for most creative applications, a battery is enough to untether from power.</p>\n<p>While the MSP430 has an fork of Arduino IDE that can be used to program the MSP430, the overhead is generally too high for the performance sensitive device.</p>\n<h3>FPGA (Honorable mention)</h3>\n<p>Field programmable gate arrays (FPGAs) are an exciting way to merge the speed of circuits with the ease of development of software.\nAlthough we will not cover FPGAs in depth, it is good to be aware of the sorts of problems that can be addressed with this tool.\nShort of designing a hardware circuit by hand from components (transistors, etc) and getting it printed onto a PCB (printed circuit board), FPGAs are the fastest tool you can get.\nThis can be useful for applications where realtime interaction (especially reaction) is important.\nOne example would be a computer vision module that moves a trash can to catch a ball of paper no matter where you throw it in the room.</p>\n<h2>Circuits</h2>\n<p>At times, we will need to build custom interfaces to the physical world.\nThis will require us to work with copper wire, resistors, sensors, and the like.\nWorking with circuits affords a huge amount of expressive freedom in terms of form factor and function, but prototyping can be a bit slow.\nThere are a number of online tools for circuit simulation that will allow you to build a simulated circuit with software, and verify that you are getting the behavior you expect.\nAn alternative development approach, and more in the spirit of the exploratory ethos of the creative embedded systems community, is to move fast and break things.\nFor the majority of components available in your average maker space, the total cost a circuit is not more than a few cents.\nThese days, it is okay if you burn a few LEDs while learning how resistors work.\nJust be sure to know when you are dealing with more expensive components, such as motor actuators or more exotic sensors.</p>\n<h3>PCB (Honorable mention)</h3>\n<p>A Printed Circuit Board (PBC) is just a more stable version of a breadboard.\nPrinting a PCB is a nice way to add polish to your system, but is more of a late-stage prototyping step.\nPCBs are first designed in software, then physically printed - usually by sending the design to a PCB manufacturer.\nSome maker spaces will have PCB mills allowing you to more quickly prototype with PCB.</p>\n<p>One difficulty to watch out for is the difference between “through hole” and SMT (surface mount technology components).\nTo start, you will want to stick with “through hole” components as they are easier to purchase and easier to use.</p>\n<p>You may be tempted to design a PCB with the raw processor that is used on your dev board.\nWhile this can shrink the form factor, you should be sure that the smaller form factor is necessary, use the processor instead of the dev board introduced a high engineering overhead.</p>\n",
      "id": 3
    },
    {
      "path": "1_Background/1_Preliminaries/3_Communication.md",
      "url": "1_Background/1_Preliminaries/3_Communication.html",
      "content": "# Communications\n\nThere are various protocols, each with their own strengths.\nHere we introduce some of the more common protocols, and consider some design tradeoffs of their application to creative embedded systems.\nWe do not describe any technical information on the implementation of these protocols, as this is readily available online.\n\n## Software Protocols\n\nSoftware protocols allow multiple software application to communicate with each other.\n\n### UDP/TCP\n\nThese protocols are the most commonly recognized due to their use as the backbone of the internet.\nIn the context of creative embedded systems, we really only need to keep in mind that UDP is faster but less reliable, whereas TCP is slower and more reliable.\nIn case you are transmitting a stream of sensor data (eg gyroscope), you are likely to prefer UDP.\nIn this case, if the UDP protocol drops a packet, we do not suffer that much, since another packet is on the way.\nIf you are transmitting event-based data (eg button presses), you are likely to prefer TCP.\n\nMost software libraries we will use in this course provide easy API to interface with these protocols.\nOne occasional difficultly to keep in mind is that, if you are manually constructing UDP or TCP packets, some network's firewalls will prevent certain kinds of these packets from being transmitted.\nIn this case, you will either need to figure out the network policy, or use a higher level communication protocol.\n\n### MIDI\n\nMIDI is both a protocol, a message format, and a hardware interface.\nFor the purposes of creative embedded systems, we again generally have useful APIs to handle constructing and passing MIDI messages between devices.\nWhile MIDI values are quite limited in the expressive power (sets of values 0-127), the high rate of adoption among musical interfaces makes it quite common in creative applications.\n\n### OSC\n\nOpenSoundControl (OSC) is more of a message format than a protocol.\nFrom an implementation perspective, it probably best to of it as a version of json specialized for music and performance.\nWe include it in the software protocols section as many creative software tools provide specialized communications APIs for OSC.\nThese APIs can can sit a top any number of the other protocol listed in this section.\n\n## Hardware Protocols\n\nAll the above software protocols rely on some hardware transmission.\nWhen working on a single device, you can completely abstract from the hardware, but as we are working with embedded systems, we must also consider how to build connections on the hardware level.\nThese connections will then allow us to communicate between devices on the software level.\n\n### Analog vs Digital\n\nThe pins on your Arduino will say both Analog and Digital.\nAnalog input allows for the reading or writing of continuous values, whereas digital is discrete (zeros and ones).\nThis is the simplest protocol you can use with embedded systems. \nThe Arduino will allow you to read and write to these pins as you like via software.\n\n### Serial bus/SPI\n\nSerial bus is the next easiest protocol to handle - all the above wired software protocols can run atop this connection.\nFor our purposes, we mostly will be able to abstract the serial bus protocol as the thing that happens when you plug in a USB.\nThat being said, there are times, particularly when interfacing with the Arduino IDE that we need to know a bit about how Serial Bus and serial ports work.\n\n### Bluetooth/BLE\n\nBluetooth, and more commonly, Bluetooth Low Energy (BLE) are standard protocols for communicating over Bluetooth devices.\n\n### i2C, UART, 1-wire, Morse code\n\nThere are a number of low level protocols for sending information over a small number of wires\nAs an example, the i<sup>2</sup>C protocol handles sending data over a two-wire interface.\nWe should not need to know anything about this for this class other than it exists and we might want to use it.\nThe implementation of these protocols is well support by many libraries.\nMore details can be found [online](https://learn.sparkfun.com/tutorials/i2c/all).\n",
      "html": "<h1>Communications</h1>\n<p>There are various protocols, each with their own strengths.\nHere we introduce some of the more common protocols, and consider some design tradeoffs of their application to creative embedded systems.\nWe do not describe any technical information on the implementation of these protocols, as this is readily available online.</p>\n<h2>Software Protocols</h2>\n<p>Software protocols allow multiple software application to communicate with each other.</p>\n<h3>UDP/TCP</h3>\n<p>These protocols are the most commonly recognized due to their use as the backbone of the internet.\nIn the context of creative embedded systems, we really only need to keep in mind that UDP is faster but less reliable, whereas TCP is slower and more reliable.\nIn case you are transmitting a stream of sensor data (eg gyroscope), you are likely to prefer UDP.\nIn this case, if the UDP protocol drops a packet, we do not suffer that much, since another packet is on the way.\nIf you are transmitting event-based data (eg button presses), you are likely to prefer TCP.</p>\n<p>Most software libraries we will use in this course provide easy API to interface with these protocols.\nOne occasional difficultly to keep in mind is that, if you are manually constructing UDP or TCP packets, some network’s firewalls will prevent certain kinds of these packets from being transmitted.\nIn this case, you will either need to figure out the network policy, or use a higher level communication protocol.</p>\n<h3>MIDI</h3>\n<p>MIDI is both a protocol, a message format, and a hardware interface.\nFor the purposes of creative embedded systems, we again generally have useful APIs to handle constructing and passing MIDI messages between devices.\nWhile MIDI values are quite limited in the expressive power (sets of values 0-127), the high rate of adoption among musical interfaces makes it quite common in creative applications.</p>\n<h3>OSC</h3>\n<p>OpenSoundControl (OSC) is more of a message format than a protocol.\nFrom an implementation perspective, it probably best to of it as a version of json specialized for music and performance.\nWe include it in the software protocols section as many creative software tools provide specialized communications APIs for OSC.\nThese APIs can can sit a top any number of the other protocol listed in this section.</p>\n<h2>Hardware Protocols</h2>\n<p>All the above software protocols rely on some hardware transmission.\nWhen working on a single device, you can completely abstract from the hardware, but as we are working with embedded systems, we must also consider how to build connections on the hardware level.\nThese connections will then allow us to communicate between devices on the software level.</p>\n<h3>Analog vs Digital</h3>\n<p>The pins on your Arduino will say both Analog and Digital.\nAnalog input allows for the reading or writing of continuous values, whereas digital is discrete (zeros and ones).\nThis is the simplest protocol you can use with embedded systems.\nThe Arduino will allow you to read and write to these pins as you like via software.</p>\n<h3>Serial bus/SPI</h3>\n<p>Serial bus is the next easiest protocol to handle - all the above wired software protocols can run atop this connection.\nFor our purposes, we mostly will be able to abstract the serial bus protocol as the thing that happens when you plug in a USB.\nThat being said, there are times, particularly when interfacing with the Arduino IDE that we need to know a bit about how Serial Bus and serial ports work.</p>\n<h3>Bluetooth/BLE</h3>\n<p>Bluetooth, and more commonly, Bluetooth Low Energy (BLE) are standard protocols for communicating over Bluetooth devices.</p>\n<h3>i2C, UART, 1-wire, Morse code</h3>\n<p>There are a number of low level protocols for sending information over a small number of wires\nAs an example, the i<sup>2</sup>C protocol handles sending data over a two-wire interface.\nWe should not need to know anything about this for this class other than it exists and we might want to use it.\nThe implementation of these protocols is well support by many libraries.\nMore details can be found <a href=\"https://learn.sparkfun.com/tutorials/i2c/all\">online</a>.</p>\n",
      "id": 4
    },
    {
      "path": "1_Background/1_Preliminaries/index.md",
      "url": "1_Background/1_Preliminaries/index.html",
      "content": "# Preliminaries\n\nThis chapter provides an introduction of the basics of the specific technologies that we will be utilizing throughout this book.\nThe material contained in this chapter is likely to change over time, as new technologies are developed and released.\n\nThis chapter may be read through for a detailed account of current technologies used in the development of embedded systems.\nAlternatively, the reader may chose to use this chapter a reference material.\nThe later chapters will focus on high level design considerations of embedded systems, and refer to this chapter as necessary for implementation details.\n\n",
      "html": "<h1>Preliminaries</h1>\n<p>This chapter provides an introduction of the basics of the specific technologies that we will be utilizing throughout this book.\nThe material contained in this chapter is likely to change over time, as new technologies are developed and released.</p>\n<p>This chapter may be read through for a detailed account of current technologies used in the development of embedded systems.\nAlternatively, the reader may chose to use this chapter a reference material.\nThe later chapters will focus on high level design considerations of embedded systems, and refer to this chapter as necessary for implementation details.</p>\n",
      "id": 5
    },
    {
      "path": "1_Background/2_Creativity/1_Whence_creativity.md",
      "url": "1_Background/2_Creativity/1_Whence_creativity.html",
      "content": "# Whence Creativity\n\nDefinitions of creativity abound and are at times significantly divergent.\nWe will adopt a supplemented version of Margaret Boden's definition as follows:\n\n>A creative process or product is one that is original, valuable or useful, and is non-obvious (surprising). [Boden, 2004]\n\nBoden states that the above definition can be contextualized and applied either to the individual involved in the creative act (_p-creativity_) or more broadly in a public, historical context she defines as _h-creativity_.\nIn other words, a creative output may be original and valuable to the creator, but in a broader historical context the output may be less valuable.\n\nThe second category, _h-creativity_ represents the highest level of creativity in many classifications because, essentially, the requirements are much more strenuous.\nFor an idea or artifact to have cultural implications requires not only mastery of the field of knowledge in which the idea or artifact is being created, but a mastery of process as well as, some argue [citation], acute societal awareness.\n\nRather than occupy ourselves with creating great works of art, our goal is to arrive at a _process_ of personal exploration where we expand our boundaries and conceptual spaces, and strive to become comfortable taking risks.\nTo facilitate this we must understand both the ideas and actions associated with creative processes.\n\n## Creativity Keyword Pairs\n\nBelow are a number of words in noun-verb pairs. They are pulled from the creative studies literature and are here meant to stimulate thought regarding art and artifact, process and product.\nIt may be non-obvious that nouns and verbs we regularly use are not directly associated with their opposite/compliment. Some may be more useful than others.\nAs we move toward implementing creative strategies (in the following section) we need to conceptually replace stasis with action.\n\nContext: _Contextualize_\n- Demonstrate understanding of, and precise relation to associated concepts\n- Action steps: purposeful reference, sample, borrowing\n\nExploration: _Explore_\n- Demonstrate/incorporate diverse inputs/concepts\n- Action steps:\n\nFlexibility: _Flex_\n- Demonstrate ability to adapt to, and overcome constraints\n- Action steps:\n\nImagination: _Imagine_\n- Demonstrate the incorporation or consideration of the non-obvious\n- Action Steps: Free Association\n\nInnovation: _Innovate_\n- Demonstrate non-conformance in context of appropriate conceptual spaces\n- Action steps:\n\nMastery: _Master_\n- Demonstrate thorough knowledge of, and incorporation of related concepts with simplicity, elegance, and power\n- Action steps:\n\nOpenness: _Open_\n- Demonstrate the taking of risks\n- Action Steps:\n- Associated concepts: newness, beyond, outside\n\nSelection: _Select_\n- Demonstrate successful selectivity of conceptual elements\n- Action steps:\n\nTransformation: _Transform_\n- Demonstrate process where concept x logically results in concept y.\n- Action steps:\n- Associated concepts: transmogrify, change, morph, redefine\n\n## Going Further: Higher level concepts/actions\n\nTruly creative and masterful works of art and music (and inventions, for that matter) demonstrate, beyond the above, significant complexity, elegance and depth. [Cite: Bob Morris]\nThis manifests in different ways, but can often be discovered through analysis where it is revealed that there are hidden strings in the work that bind one element to another.\nThese are rarely obvious, and thus demonstrate one manifestation of the requirements for true creativity, surprise.\n\nFurther, what we do _not_ see in the above is any notion of \"correctness\". Right and wrong and associated patterns of black and white thinking are anathema to the creative process where it is not unusual to actually begin with a nonsensical or \"bad\" idea in order to arrive at something unusual or new.\nThis is difficult to accept, especially in a technical context where proper engineering plays a significant role in the production of the idea or artifact.\nTo be successful we must separate the two processes, the creative conceptualization and the implementation. This is not to say implementations themselves cannot be creative, but that they must also be \"correct\" in that they \"work\" to realize the idea or artifact.\nWhen the artifact is realized, the implementation is \"correct\" and the elegance of the implementation reinforces the depth of the creative process.\n\nWe will actively explore certain noun-verb pairs and their associated concepts and actions in the different Modules in Section 2.\n",
      "html": "<h1>Whence Creativity</h1>\n<p>Definitions of creativity abound and are at times significantly divergent.\nWe will adopt a supplemented version of Margaret Boden’s definition as follows:</p>\n<blockquote>\n<p>A creative process or product is one that is original, valuable or useful, and is non-obvious (surprising). [Boden, 2004]</p>\n</blockquote>\n<p>Boden states that the above definition can be contextualized and applied either to the individual involved in the creative act (<em>p-creativity</em>) or more broadly in a public, historical context she defines as <em>h-creativity</em>.\nIn other words, a creative output may be original and valuable to the creator, but in a broader historical context the output may be less valuable.</p>\n<p>The second category, <em>h-creativity</em> represents the highest level of creativity in many classifications because, essentially, the requirements are much more strenuous.\nFor an idea or artifact to have cultural implications requires not only mastery of the field of knowledge in which the idea or artifact is being created, but a mastery of process as well as, some argue [citation], acute societal awareness.</p>\n<p>Rather than occupy ourselves with creating great works of art, our goal is to arrive at a <em>process</em> of personal exploration where we expand our boundaries and conceptual spaces, and strive to become comfortable taking risks.\nTo facilitate this we must understand both the ideas and actions associated with creative processes.</p>\n<h2>Creativity Keyword Pairs</h2>\n<p>Below are a number of words in noun-verb pairs. They are pulled from the creative studies literature and are here meant to stimulate thought regarding art and artifact, process and product.\nIt may be non-obvious that nouns and verbs we regularly use are not directly associated with their opposite/compliment. Some may be more useful than others.\nAs we move toward implementing creative strategies (in the following section) we need to conceptually replace stasis with action.</p>\n<p>Context: <em>Contextualize</em></p>\n<ul>\n<li>Demonstrate understanding of, and precise relation to associated concepts</li>\n<li>Action steps: purposeful reference, sample, borrowing</li>\n</ul>\n<p>Exploration: <em>Explore</em></p>\n<ul>\n<li>Demonstrate/incorporate diverse inputs/concepts</li>\n<li>Action steps:</li>\n</ul>\n<p>Flexibility: <em>Flex</em></p>\n<ul>\n<li>Demonstrate ability to adapt to, and overcome constraints</li>\n<li>Action steps:</li>\n</ul>\n<p>Imagination: <em>Imagine</em></p>\n<ul>\n<li>Demonstrate the incorporation or consideration of the non-obvious</li>\n<li>Action Steps: Free Association</li>\n</ul>\n<p>Innovation: <em>Innovate</em></p>\n<ul>\n<li>Demonstrate non-conformance in context of appropriate conceptual spaces</li>\n<li>Action steps:</li>\n</ul>\n<p>Mastery: <em>Master</em></p>\n<ul>\n<li>Demonstrate thorough knowledge of, and incorporation of related concepts with simplicity, elegance, and power</li>\n<li>Action steps:</li>\n</ul>\n<p>Openness: <em>Open</em></p>\n<ul>\n<li>Demonstrate the taking of risks</li>\n<li>Action Steps:</li>\n<li>Associated concepts: newness, beyond, outside</li>\n</ul>\n<p>Selection: <em>Select</em></p>\n<ul>\n<li>Demonstrate successful selectivity of conceptual elements</li>\n<li>Action steps:</li>\n</ul>\n<p>Transformation: <em>Transform</em></p>\n<ul>\n<li>Demonstrate process where concept x logically results in concept y.</li>\n<li>Action steps:</li>\n<li>Associated concepts: transmogrify, change, morph, redefine</li>\n</ul>\n<h2>Going Further: Higher level concepts/actions</h2>\n<p>Truly creative and masterful works of art and music (and inventions, for that matter) demonstrate, beyond the above, significant complexity, elegance and depth. [Cite: Bob Morris]\nThis manifests in different ways, but can often be discovered through analysis where it is revealed that there are hidden strings in the work that bind one element to another.\nThese are rarely obvious, and thus demonstrate one manifestation of the requirements for true creativity, surprise.</p>\n<p>Further, what we do <em>not</em> see in the above is any notion of “correctness”. Right and wrong and associated patterns of black and white thinking are anathema to the creative process where it is not unusual to actually begin with a nonsensical or “bad” idea in order to arrive at something unusual or new.\nThis is difficult to accept, especially in a technical context where proper engineering plays a significant role in the production of the idea or artifact.\nTo be successful we must separate the two processes, the creative conceptualization and the implementation. This is not to say implementations themselves cannot be creative, but that they must also be “correct” in that they “work” to realize the idea or artifact.\nWhen the artifact is realized, the implementation is “correct” and the elegance of the implementation reinforces the depth of the creative process.</p>\n<p>We will actively explore certain noun-verb pairs and their associated concepts and actions in the different Modules in Section 2.</p>\n",
      "id": 6
    },
    {
      "path": "1_Background/2_Creativity/2_Presenting_your_work.md",
      "url": "1_Background/2_Creativity/2_Presenting_your_work.html",
      "content": "# Presenting your work\n\nDeveloping skill and creativity in your craft of choice is only the first step of artistic practice - you must also be able to disseminate your ideas to your audience.\nThe medium through which you present your work is itself a creative act.\nThis is especially true in the domain of embedded systems, as your devices and installations are physically limited in the distribution.\nIn order to reach a wider audience, you will need to capture a record of your work.\nRather than considering this record as a lesser representation of the original, think of this record as a creative act in and of itself.\n\n## Documenting a work\n\nWe consider two types of records of a creative work.\nThe first, documentation, is generally a more technical style, where the primary purpose is to ensure the work is reproducible.\nOn the other hand, what we call Artistic records, attempt to capture the artistic side of the work.\n\nAs an example of the highest quality documentation, see the [NSynth project](https://github.com/googlecreativelab/open-nsynth-super).\nWhile this is not a reasonable standard to hold yourself to for every project, this is a good demonstration of what documenting work looks like at its best.\n\n### Documentation\n\nThe majority of documentation for a creative embedded system will be similar to documenting any other embedded system.\nProvide as much detail as possible on the hardware setup (through, for example, circuit diagrams) as well as software requirements, such as library dependencies.\nIf your system has some server side functionality, using Continuous Integration tools can help to ensure reproducibility of the system.\nVirtual machines, or their lighter weight cousin Docker files, are also an effective way to ensure reproducibility.\n\n### Artistic Records\n\nGenerating a record of the artistic ethos of your work is a more subjective task.\nIn the space of embedded systems, one of the most effective ways to do this is through video demos.\nWhile a video with high production quality does present a more polished view of your work, even a video taking with your smartphone is often sufficient to capture the basic idea.\nThere is a vast difference in how well an audience can understand between a work between a smartphone video and no video at all.\n\n**Building a portfolio**\n\nAs you are documenting your work, it is important to keep all of these records in a single space.\nAs you build more projects, this single space will become your digital portfolio.\nIf you are planning to work on creative systems in a professional capacity, this portfolio can be a valuable resource to communicate to potential patrons your capabilities and style.\nOne place to easily collect your work in on GitHub Pages, a way to host a free static website through GitHub.\nWe will cover the matter of producing a personal digital portfolio in class.\n\n\n",
      "html": "<h1>Presenting your work</h1>\n<p>Developing skill and creativity in your craft of choice is only the first step of artistic practice - you must also be able to disseminate your ideas to your audience.\nThe medium through which you present your work is itself a creative act.\nThis is especially true in the domain of embedded systems, as your devices and installations are physically limited in the distribution.\nIn order to reach a wider audience, you will need to capture a record of your work.\nRather than considering this record as a lesser representation of the original, think of this record as a creative act in and of itself.</p>\n<h2>Documenting a work</h2>\n<p>We consider two types of records of a creative work.\nThe first, documentation, is generally a more technical style, where the primary purpose is to ensure the work is reproducible.\nOn the other hand, what we call Artistic records, attempt to capture the artistic side of the work.</p>\n<p>As an example of the highest quality documentation, see the <a href=\"https://github.com/googlecreativelab/open-nsynth-super\">NSynth project</a>.\nWhile this is not a reasonable standard to hold yourself to for every project, this is a good demonstration of what documenting work looks like at its best.</p>\n<h3>Documentation</h3>\n<p>The majority of documentation for a creative embedded system will be similar to documenting any other embedded system.\nProvide as much detail as possible on the hardware setup (through, for example, circuit diagrams) as well as software requirements, such as library dependencies.\nIf your system has some server side functionality, using Continuous Integration tools can help to ensure reproducibility of the system.\nVirtual machines, or their lighter weight cousin Docker files, are also an effective way to ensure reproducibility.</p>\n<h3>Artistic Records</h3>\n<p>Generating a record of the artistic ethos of your work is a more subjective task.\nIn the space of embedded systems, one of the most effective ways to do this is through video demos.\nWhile a video with high production quality does present a more polished view of your work, even a video taking with your smartphone is often sufficient to capture the basic idea.\nThere is a vast difference in how well an audience can understand between a work between a smartphone video and no video at all.</p>\n<p><strong>Building a portfolio</strong></p>\n<p>As you are documenting your work, it is important to keep all of these records in a single space.\nAs you build more projects, this single space will become your digital portfolio.\nIf you are planning to work on creative systems in a professional capacity, this portfolio can be a valuable resource to communicate to potential patrons your capabilities and style.\nOne place to easily collect your work in on GitHub Pages, a way to host a free static website through GitHub.\nWe will cover the matter of producing a personal digital portfolio in class.</p>\n",
      "id": 7
    },
    {
      "path": "1_Background/2_Creativity/index.md",
      "url": "1_Background/2_Creativity/index.html",
      "content": "# Creativity\n\nCreation and creativity are at the core of many disciplines. \nHowever, while there is general agreement about what creativity and creative artifacts are, there is no established, agreed-upon definition of *how* creative people work nor precisely how to evaluate their creative output. \nThis is partially because artifacts and the expertise to evaluate them vary dramatically across domains.\n\nThere are two fields of thought on this problem, those that believe that the creative processes is universal and a general set of criteria exist to evaluate creative endeavors regardless of domain, and those that believe the creative process itself differs from domain to domain. \nThese are described as Domain-General Creativity and Domain-Specific Creativity respectively.\n\nFortunately, we can study common traits and factors that feature in creative processes without concerning ourselves with the validity of either argument. \nThese factors include ones natural skills and abilities as well as what one has learned through study, individual character traits and motivations, and how one learns and thinks. [citation CHCAD] \nAll of these manifest in individual modes of working and creating that are highly individual. \nBecause of this, we believe that creativity and what it means to be creative in our work is something to be explored and experienced through practical application. \nThus, our concern is twofold: what is creativity broadly and how can we become more creative in our own work.\n",
      "html": "<h1>Creativity</h1>\n<p>Creation and creativity are at the core of many disciplines.\nHowever, while there is general agreement about what creativity and creative artifacts are, there is no established, agreed-upon definition of <em>how</em> creative people work nor precisely how to evaluate their creative output.\nThis is partially because artifacts and the expertise to evaluate them vary dramatically across domains.</p>\n<p>There are two fields of thought on this problem, those that believe that the creative processes is universal and a general set of criteria exist to evaluate creative endeavors regardless of domain, and those that believe the creative process itself differs from domain to domain.\nThese are described as Domain-General Creativity and Domain-Specific Creativity respectively.</p>\n<p>Fortunately, we can study common traits and factors that feature in creative processes without concerning ourselves with the validity of either argument.\nThese factors include ones natural skills and abilities as well as what one has learned through study, individual character traits and motivations, and how one learns and thinks. [citation CHCAD]\nAll of these manifest in individual modes of working and creating that are highly individual.\nBecause of this, we believe that creativity and what it means to be creative in our work is something to be explored and experienced through practical application.\nThus, our concern is twofold: what is creativity broadly and how can we become more creative in our own work.</p>\n",
      "id": 8
    },
    {
      "path": "2_Modules/1_Generative_Art/index.md",
      "url": "2_Modules/1_Generative_Art/index.html",
      "content": "# Generative Art\n\n## System Configuration\n\n\n              |------|\n              |  mp  | ---> D_out\n              |------|  \n\n\nThis is the simplest class of systems we will address - a microprocessor with digital out.\nWe address two options for digital out - either visual or audio.\nThis captures well the domain of generative art - the practice of using automation to create content.\nGenerative works are those created with the assistance of a program that significantly, sometimes entirely, informs the composition and realization of the work.\n\nGenerative art can be static (complete) or dynamic (iterative).\nIn dynamic generative art the generative *process* is treated as content, rather than static content produced by the process.\nSome examples of static content includes using an algorithm to generative designs for scarfs, that are then custom knitted.\nWhile this falls into the category of generative art, the content is the art, whereas the process is not, in this case, presented to the audience.\n\nIn contrast, in dynamic generative art the process of generation is visible to the audience.\nWe provide some examples below of this style of generative art.\nWe note that we are particularly interested in generative processes that fit into a semi-automated definition - by some process, random or otherwise, the output of the system at any given time should not be predictable based solely on the initial state.\nThe generative process evolves in a way that cannot be captured by fixed media.\n\n### Generative Art vs Visualizations\n\nCan a data visualization be considered generative art?\nCan all data visualizations be considered generative art?\nUnderstanding these classifications can help us to make more directed creative choices in our art.\nWhile data visualizations and generative art are certainly not mutually exclusive, there is a difference in ethos between the two.\nWhereas data visualizations are conceptually driven by the data ingest, generative art is conceptually driven by the creative choices of the artists.\nIn generative art, the process is designed to be self-consuming, generally using its own internal state to evolve.\nData ingest may play a role in how the artist chooses to evolve the state of the system, but it does not (generally) dominate.\n\n\n---------------\n\n## Application Domains\n\n---------------\n\n### Live Coding \n\nIn our taxonomy of systems, we allow audio or visuals as D<sub>out</sub>.\nOne creative practice that falls into this category is *live coding*.\nLive Coding is an artistic practice where the act of programming becomes a critical part of the end product.\nRather than creating a static program that is deployed to users after it is complete, live coding shifts this perspective.\nIt is the process of creating the software that is shown now users.\nNick Collins, a pioneer of live coding, captures this sentiment well in giving a description of live coding:\n\n> “Art of re-programming; changing your mind about a process once established” <br> <div align=\"right\">*-Nick Collins*</div>\n\nThe practice of live coding can be more formalism on a stage, where the music produced is more beat oriented.\nIn this example, Sam Aaron, the developer of Sonic Pi, demonstrates his live coding practice.\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/0dgZf3Com44?start=1162\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\nFor live coding, we do require some input from the artist (in the form of a keyboard and mouse), however the act of providing input is not \"visible\" to the viewer.\nTo say the input is not visible to the viewer is not to say it is not an integral part of the performance.\nIndeed, one of the oft repeated mantras of live coding is \"show your code\".\nAllowing the viewer to watch the source of the live coded content evolve is a defining feature of the practice of live coding.\nHowever, what is shown to the viewer is the code on the screen - not (generally) the physical process of inputting the code.\nFrom the perspective of an embedded system, we are not making the input modality a part of the system.\nWe will revisit the issue of physically visible input in Module 2, [Interactive Devices](../2_Interactive_Devices/index.md).\n\n#### Audio Live Coding \n\nYour Raspberry Pi (with Raspbian OS) comes pre-installed with a program called Sonic Pi, a language for live coding audio.\nYou can start exploring the practice of live coding with this tool.\nA tutorial is available within the Sonic Pi IDE.\nLive coding does not strictly follow the system configuration described above, as we also use an input to the system from the performer.\n\nIn order to use Sonic Pi for generative art in the system configuration we present in this chapter, we need to start a Sonic Pi program when the device boots.\nDetails on this procedure are provided in [Appendix](../../Appendix/Run_on_boot.md).\nStarting a generative art program during the boot sequence of the embedded device is a standard way to package your project into a self-contained and easy to use installation.\n\nAnother language for live coding audio is [Tidal](https://tidalcycles.org/), which is an embedded DSL in Haskell.\nAs opposed to Sonic Pi, which is beat oriented, Tidal is pattern oriented, which tends to lead artists down a different creative path.\nIt is also possible to live code in [SuperCollider](https://toplap.org/howto_co34pt_livecode-resources-about-how-i-live-code-in-supercollider/), which tends to be more signal oriented.\n\nThese live coding languages can also be used to create generative audio processes that do not require input from the artist.\nFor example, you might have a Sonic Pi program pre-written that algorithmically generates music.\nYou could [start that script on boot](../../Appendix/Run_on_boot.md), so the audio begins playing as soon as the Pi is powered on.\n\n#### Visual Live Coding\n\nLive coding for visuals is also an active space of exploration.\nIn this section, we will focus on D<sub>out</sub> as only visuals.\nIn the space of live coding as an artistic practice, visualists are often separate performers from the audio live coders.\nConsequentially, many performances of live coding with visuals will be duets, where the two performers collaborate on stage.\n\nIn order to quickly test out some live coding for visuals, the programming language [Hydra](https://hydra-editor.glitch.me) is available online.\n\nThe practice of live coding can also be more informal, as in this example, which is set in a less formalized environment.\nThis set is presented by Alex McLean on audio and Dan Hett on visuals.\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/fIuqDKzYBzc\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\nSimilar to audio, you could use a tool like Hydra to generate visuals, and [start that script on boot](../../Appendix/Run_on_boot.md), so the visuals immediately start when the Pi is powered on.\n\n---------------\n\n## Kiosk Installation Art\n\nWe use the term _kiosk installation art_ to capture standalone generative art installations that do not take input from the viewer.\nThis type of installation is captured well by the Generative Art system configuration described above.\nSuch installations are often found as stations in museums, but are also often found in public spaces.\n\nThe space in which kiosk installation art is to be displayed should be an important design consideration.\nIf a museum, you might expect a longer term engagement with each audience members.\nIn contrast, in a public space, more audience members are likely to engage with your art for shorter periods of time, perhaps as they walk by the display.\nIn the below example of the LED display in the Becton Ground Cafe at Yale University, the majority of users will engage with the work while passing by the building, some will engage as they are eating or working in the cafe, and small minority will enter the cafe specifically to view your work.\nIt is again a creative choice to decide which subset of the audience you are primarily designing for, or if in fact there is a dichotomy between these groups at all.\n\nAs a note, some installation art will rely on input from the viewer.\nThe viewer may contribute to the art knowingly (actively) or unknowingly (passively).\nWe will not draw a formal distinction between these two types of input as, from a technical perspective, we must solve similar issues.\nWe revisit issues of interactivity in future modules.\n\n### Multichannel displays\n\n![ceid cafe video display](../../ExternalResources/Media/ceid_cafe.jpg)\n\n> An example of a video installation exhibit at Yale's Becton Ground Cafe.\n\n![columbia video display](../../ExternalResources/Media/columbia_video.jpg)\n\n> Columbia University's Morningside Heights Campus at the Greene Science Center also features a video wall.\n\nOne type of kiosk installation art is multichannel displays.\nIn a multichannel video display, there are multiple displays available to the artist.\nIn the simplest case, the displays have the same virtual layout as their physical counterpart.\nHowever, in an increasing number of cases, the connection between the virtual layout and the physical layout is disjoint.\nThis presents a challenge/opportunity for the artist to work within the dissonance of the virtual and physical worlds.\n\n\n\n---------------\n\n## Assignment\n\nDue Jan 29th, 11:59pm\n\n### Generative Kiosk Art (20 pts)\n\nIn this lab you will use your:\n\n- Raspberry Pi\n- external monitor\n- Freenove 8 RGB LED module\n\nThe goal is to create a generative kiosk art installation in your home that faces outward.\nYou will place your monitor and LED module in a window, and create a visualization that fits this context.\nUse this opportunity to reflect on the connection between your living space, the outside world, and how we have experienced this divide in the past year.\n\nNOTE: proceed with caution, but don't be scared! \nYou are displaying art (possibly for the first time) to the public. \nConsider: who will see this, what will they see, when will they see it?\nYou may choose to put up your installation for only a short time (long enough for you to document your installation).\n\nFrom a software perspective, you will write a \n\n- python script to control the LED module that utilizes data from a web API of your choice\n- a processing script to control the external monitor\n- edit the file to start both of these scripts on boot\n\n#### Task #1 \n\nIn Task #1, you will write a generative visualization in Processing. You need to get this code onto your Pi, and set up the Pi so that your processing script runs on boot, in full screen mode.\n\nWe will lay the foundations for task on Friday lab, 1/15.\n\n#### Task #2\n\nGet the 8 RGB LED module connected to the Raspberry Pi.\nUse python to control the LEDs.\nThe LED module you have follows the same configuration as a Neopixel module.\nYou should mostly follow the sample code is here. [https://learn.adafruit.com/neopixels-on-raspberry-pi/overview](https://learn.adafruit.com/neopixels-on-raspberry-pi/overview).\n\nWe will start this task on Friday lab, 1/22.\n\n---------------\n\nThis is the first open-ended, creatively driven project of the class. \nThere are two main goals.\nThe first goal is to introduce you to the challenge of working with an underspecified problem - modelling the typical real world constraints given to installation artists of: “make something for this space”. \nThe second goal is to get you comfortable with showing you art to the world in a low stakes way.\n\nSubmit a link to your blog post on the course blog. That post should contain:\n\n(10 pts total - see below for breakdown) A link to your git repository with a program that runs on a Raspberry Pi to generate a visual for your display. The program must meet the following criteria:\n\n- (3 pts) Be generative as defined in the course text (now you have to at least skim the reading)\n- (2 pts) Start on boot of the Raspberry Pi, and display fullscreen (for processing).\n- (2 pts) Call a web API.\n- (2 pts) LEDs must turn on and respond to your code.\n- (1 pts) Is in the spirit of the class as broadly interpreted by the instructors. Art is subjective, we want you to get comfortable with this ethos. \n\n#### Standard Documentation Deliverables:\n\n(10 pts total - see below for breakdown)\n\nIn addition to the project specific deliverables lists above, you must also meet the following “standard documentation deliverables”. Throughout this course, we will ask you to document your work in order to slowly build a portfolio of your projects. Going forward, these types of standard documentation deliverables can be assumed to be required for all assignments unless specified otherwise. \n\n(5 pts) *A blog post* \n\nUsing the blog site (more info to come), make a blog post describing your art. \nThe post should give an overview of your artistic vision. \nIn particular for this assignment, you should address how you have specialized your generative art to the space.\nWhat creative decisions did you work lead you to, and which decisions did you take?\nHow were your decisions motivated by your larger creative vision for this project.\nIn the same vein, also address any technical issues you encountered in your work.\nParticularly focus on issues that other artists may encounter when developing with your hardware setup.\n\n(3 pts) *A README* \n\nOn your github repo add a readme that contains a short description and key information on reproducibility/installation/usage.\nThis key information should be sufficient for a knowledge third party, outside the class, to replicate your design.\nThis readme can/should be a subset of the material used in your CoursePress blog post.\n\n(2 pts) *A video of your art*\n\nInclude in the README a link to your video. The video can be a simple video shot on your phone - the only goal is to have a record of your art in action. You can host the video wherever you like as long as the hosting platform supports in-browser playback (e.g. YouTube, Vimeo). You may also choose to embed a gif in your README in place of a video link.\n\nIf you cannot access a public space from which your window is visible, please message me directly. \n\n",
      "html": "<h1>Generative Art</h1>\n<h2>System Configuration</h2>\n<pre><code>          |------|\n          |  mp  | ---&gt; D_out\n          |------|  \n</code></pre>\n<p>This is the simplest class of systems we will address - a microprocessor with digital out.\nWe address two options for digital out - either visual or audio.\nThis captures well the domain of generative art - the practice of using automation to create content.\nGenerative works are those created with the assistance of a program that significantly, sometimes entirely, informs the composition and realization of the work.</p>\n<p>Generative art can be static (complete) or dynamic (iterative).\nIn dynamic generative art the generative <em>process</em> is treated as content, rather than static content produced by the process.\nSome examples of static content includes using an algorithm to generative designs for scarfs, that are then custom knitted.\nWhile this falls into the category of generative art, the content is the art, whereas the process is not, in this case, presented to the audience.</p>\n<p>In contrast, in dynamic generative art the process of generation is visible to the audience.\nWe provide some examples below of this style of generative art.\nWe note that we are particularly interested in generative processes that fit into a semi-automated definition - by some process, random or otherwise, the output of the system at any given time should not be predictable based solely on the initial state.\nThe generative process evolves in a way that cannot be captured by fixed media.</p>\n<h3>Generative Art vs Visualizations</h3>\n<p>Can a data visualization be considered generative art?\nCan all data visualizations be considered generative art?\nUnderstanding these classifications can help us to make more directed creative choices in our art.\nWhile data visualizations and generative art are certainly not mutually exclusive, there is a difference in ethos between the two.\nWhereas data visualizations are conceptually driven by the data ingest, generative art is conceptually driven by the creative choices of the artists.\nIn generative art, the process is designed to be self-consuming, generally using its own internal state to evolve.\nData ingest may play a role in how the artist chooses to evolve the state of the system, but it does not (generally) dominate.</p>\n<hr>\n<h2>Application Domains</h2>\n<hr>\n<h3>Live Coding</h3>\n<p>In our taxonomy of systems, we allow audio or visuals as D<sub>out</sub>.\nOne creative practice that falls into this category is <em>live coding</em>.\nLive Coding is an artistic practice where the act of programming becomes a critical part of the end product.\nRather than creating a static program that is deployed to users after it is complete, live coding shifts this perspective.\nIt is the process of creating the software that is shown now users.\nNick Collins, a pioneer of live coding, captures this sentiment well in giving a description of live coding:</p>\n<blockquote>\n<p>“Art of re-programming; changing your mind about a process once established” <br> <div align=\"right\"><em>-Nick Collins</em></div></p>\n</blockquote>\n<p>The practice of live coding can be more formalism on a stage, where the music produced is more beat oriented.\nIn this example, Sam Aaron, the developer of Sonic Pi, demonstrates his live coding practice.</p>\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/0dgZf3Com44?start=1162\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n<p>For live coding, we do require some input from the artist (in the form of a keyboard and mouse), however the act of providing input is not “visible” to the viewer.\nTo say the input is not visible to the viewer is not to say it is not an integral part of the performance.\nIndeed, one of the oft repeated mantras of live coding is “show your code”.\nAllowing the viewer to watch the source of the live coded content evolve is a defining feature of the practice of live coding.\nHowever, what is shown to the viewer is the code on the screen - not (generally) the physical process of inputting the code.\nFrom the perspective of an embedded system, we are not making the input modality a part of the system.\nWe will revisit the issue of physically visible input in Module 2, <a href=\"../2_Interactive_Devices/index.html\">Interactive Devices</a>.</p>\n<h4>Audio Live Coding</h4>\n<p>Your Raspberry Pi (with Raspbian OS) comes pre-installed with a program called Sonic Pi, a language for live coding audio.\nYou can start exploring the practice of live coding with this tool.\nA tutorial is available within the Sonic Pi IDE.\nLive coding does not strictly follow the system configuration described above, as we also use an input to the system from the performer.</p>\n<p>In order to use Sonic Pi for generative art in the system configuration we present in this chapter, we need to start a Sonic Pi program when the device boots.\nDetails on this procedure are provided in <a href=\"../../Appendix/Run_on_boot.html\">Appendix</a>.\nStarting a generative art program during the boot sequence of the embedded device is a standard way to package your project into a self-contained and easy to use installation.</p>\n<p>Another language for live coding audio is <a href=\"https://tidalcycles.org/\">Tidal</a>, which is an embedded DSL in Haskell.\nAs opposed to Sonic Pi, which is beat oriented, Tidal is pattern oriented, which tends to lead artists down a different creative path.\nIt is also possible to live code in <a href=\"https://toplap.org/howto_co34pt_livecode-resources-about-how-i-live-code-in-supercollider/\">SuperCollider</a>, which tends to be more signal oriented.</p>\n<p>These live coding languages can also be used to create generative audio processes that do not require input from the artist.\nFor example, you might have a Sonic Pi program pre-written that algorithmically generates music.\nYou could <a href=\"../../Appendix/Run_on_boot.html\">start that script on boot</a>, so the audio begins playing as soon as the Pi is powered on.</p>\n<h4>Visual Live Coding</h4>\n<p>Live coding for visuals is also an active space of exploration.\nIn this section, we will focus on D<sub>out</sub> as only visuals.\nIn the space of live coding as an artistic practice, visualists are often separate performers from the audio live coders.\nConsequentially, many performances of live coding with visuals will be duets, where the two performers collaborate on stage.</p>\n<p>In order to quickly test out some live coding for visuals, the programming language <a href=\"https://hydra-editor.glitch.me\">Hydra</a> is available online.</p>\n<p>The practice of live coding can also be more informal, as in this example, which is set in a less formalized environment.\nThis set is presented by Alex McLean on audio and Dan Hett on visuals.</p>\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/fIuqDKzYBzc\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n<p>Similar to audio, you could use a tool like Hydra to generate visuals, and <a href=\"../../Appendix/Run_on_boot.html\">start that script on boot</a>, so the visuals immediately start when the Pi is powered on.</p>\n<hr>\n<h2>Kiosk Installation Art</h2>\n<p>We use the term <em>kiosk installation art</em> to capture standalone generative art installations that do not take input from the viewer.\nThis type of installation is captured well by the Generative Art system configuration described above.\nSuch installations are often found as stations in museums, but are also often found in public spaces.</p>\n<p>The space in which kiosk installation art is to be displayed should be an important design consideration.\nIf a museum, you might expect a longer term engagement with each audience members.\nIn contrast, in a public space, more audience members are likely to engage with your art for shorter periods of time, perhaps as they walk by the display.\nIn the below example of the LED display in the Becton Ground Cafe at Yale University, the majority of users will engage with the work while passing by the building, some will engage as they are eating or working in the cafe, and small minority will enter the cafe specifically to view your work.\nIt is again a creative choice to decide which subset of the audience you are primarily designing for, or if in fact there is a dichotomy between these groups at all.</p>\n<p>As a note, some installation art will rely on input from the viewer.\nThe viewer may contribute to the art knowingly (actively) or unknowingly (passively).\nWe will not draw a formal distinction between these two types of input as, from a technical perspective, we must solve similar issues.\nWe revisit issues of interactivity in future modules.</p>\n<h3>Multichannel displays</h3>\n<p><img src=\"../../ExternalResources/Media/ceid_cafe.jpg\" alt=\"ceid cafe video display\"></p>\n<blockquote>\n<p>An example of a video installation exhibit at Yale’s Becton Ground Cafe.</p>\n</blockquote>\n<p><img src=\"../../ExternalResources/Media/columbia_video.jpg\" alt=\"columbia video display\"></p>\n<blockquote>\n<p>Columbia University’s Morningside Heights Campus at the Greene Science Center also features a video wall.</p>\n</blockquote>\n<p>One type of kiosk installation art is multichannel displays.\nIn a multichannel video display, there are multiple displays available to the artist.\nIn the simplest case, the displays have the same virtual layout as their physical counterpart.\nHowever, in an increasing number of cases, the connection between the virtual layout and the physical layout is disjoint.\nThis presents a challenge/opportunity for the artist to work within the dissonance of the virtual and physical worlds.</p>\n<hr>\n<h2>Assignment</h2>\n<p>Due Jan 29th, 11:59pm</p>\n<h3>Generative Kiosk Art (20 pts)</h3>\n<p>In this lab you will use your:</p>\n<ul>\n<li>Raspberry Pi</li>\n<li>external monitor</li>\n<li>Freenove 8 RGB LED module</li>\n</ul>\n<p>The goal is to create a generative kiosk art installation in your home that faces outward.\nYou will place your monitor and LED module in a window, and create a visualization that fits this context.\nUse this opportunity to reflect on the connection between your living space, the outside world, and how we have experienced this divide in the past year.</p>\n<p>NOTE: proceed with caution, but don’t be scared!\nYou are displaying art (possibly for the first time) to the public.\nConsider: who will see this, what will they see, when will they see it?\nYou may choose to put up your installation for only a short time (long enough for you to document your installation).</p>\n<p>From a software perspective, you will write a</p>\n<ul>\n<li>python script to control the LED module that utilizes data from a web API of your choice</li>\n<li>a processing script to control the external monitor</li>\n<li>edit the file to start both of these scripts on boot</li>\n</ul>\n<h4>Task #1</h4>\n<p>In Task #1, you will write a generative visualization in Processing. You need to get this code onto your Pi, and set up the Pi so that your processing script runs on boot, in full screen mode.</p>\n<p>We will lay the foundations for task on Friday lab, 1/15.</p>\n<h4>Task #2</h4>\n<p>Get the 8 RGB LED module connected to the Raspberry Pi.\nUse python to control the LEDs.\nThe LED module you have follows the same configuration as a Neopixel module.\nYou should mostly follow the sample code is here. <a href=\"https://learn.adafruit.com/neopixels-on-raspberry-pi/overview\">https://learn.adafruit.com/neopixels-on-raspberry-pi/overview</a>.</p>\n<p>We will start this task on Friday lab, 1/22.</p>\n<hr>\n<p>This is the first open-ended, creatively driven project of the class.\nThere are two main goals.\nThe first goal is to introduce you to the challenge of working with an underspecified problem - modelling the typical real world constraints given to installation artists of: “make something for this space”.\nThe second goal is to get you comfortable with showing you art to the world in a low stakes way.</p>\n<p>Submit a link to your blog post on the course blog. That post should contain:</p>\n<p>(10 pts total - see below for breakdown) A link to your git repository with a program that runs on a Raspberry Pi to generate a visual for your display. The program must meet the following criteria:</p>\n<ul>\n<li>(3 pts) Be generative as defined in the course text (now you have to at least skim the reading)</li>\n<li>(2 pts) Start on boot of the Raspberry Pi, and display fullscreen (for processing).</li>\n<li>(2 pts) Call a web API.</li>\n<li>(2 pts) LEDs must turn on and respond to your code.</li>\n<li>(1 pts) Is in the spirit of the class as broadly interpreted by the instructors. Art is subjective, we want you to get comfortable with this ethos.</li>\n</ul>\n<h4>Standard Documentation Deliverables:</h4>\n<p>(10 pts total - see below for breakdown)</p>\n<p>In addition to the project specific deliverables lists above, you must also meet the following “standard documentation deliverables”. Throughout this course, we will ask you to document your work in order to slowly build a portfolio of your projects. Going forward, these types of standard documentation deliverables can be assumed to be required for all assignments unless specified otherwise.</p>\n<p>(5 pts) <em>A blog post</em></p>\n<p>Using the blog site (more info to come), make a blog post describing your art.\nThe post should give an overview of your artistic vision.\nIn particular for this assignment, you should address how you have specialized your generative art to the space.\nWhat creative decisions did you work lead you to, and which decisions did you take?\nHow were your decisions motivated by your larger creative vision for this project.\nIn the same vein, also address any technical issues you encountered in your work.\nParticularly focus on issues that other artists may encounter when developing with your hardware setup.</p>\n<p>(3 pts) <em>A README</em></p>\n<p>On your github repo add a readme that contains a short description and key information on reproducibility/installation/usage.\nThis key information should be sufficient for a knowledge third party, outside the class, to replicate your design.\nThis readme can/should be a subset of the material used in your CoursePress blog post.</p>\n<p>(2 pts) <em>A video of your art</em></p>\n<p>Include in the README a link to your video. The video can be a simple video shot on your phone - the only goal is to have a record of your art in action. You can host the video wherever you like as long as the hosting platform supports in-browser playback (e.g. YouTube, Vimeo). You may also choose to embed a gif in your README in place of a video link.</p>\n<p>If you cannot access a public space from which your window is visible, please message me directly.</p>\n",
      "id": 9
    },
    {
      "path": "2_Modules/2_Interactive_Devices/index.md",
      "url": "2_Modules/2_Interactive_Devices/index.html",
      "content": "# Interactive Devices\n\n## System Configuration\n\n              |------|\n              |  mp  | ---> D_out\n              |------|  \n                 ^\n                 |\n                 |\n                 | \n                 v\n              |------|\n    A_in ---> |  mc  |\n              |------|  \n\nIn this module, we extend our previous configuration with analog input.\nWe use the term analog input to capture the class of circuit designs that allow us to pull data from the real (analog) world into the digital world.\nAccordingly, we now include A<sub>in</sub> (analog input) in our system configuration specification.\n\n\n### Digital vs Analog Input\n\nIn addition to analog input, we now also include digital input in our systems.\nIn fact, the distinction between digital and analog input is somewhat arbitrary.\nDigital input is voltage reduced to either high (1) or low (0) value on your computation device.\nIn practice, a high digital signal value is an analog signal value with a value near the top of the voltage range, while a low digital is a voltage signal near the bottom of the range.\nSimilarly, analog input is voltage reduced to the space of the number of bits your computation device support on its digital pins.\n\nFrom a hardware perspective, the Raspberry Pi includes a set of GPIO pins that allow you to interface with circuits.\nWithout an ADC (analog to digital converter, for input) or DAC (digital to analog converter, for output), you cannot use the Raspberry Pi for analog signals - all signals are digital (0 or 1).\nThis is not to say that you cannot use analog input devices, just that they will be reduced to digital inputs.\nAt times this will be the most appropriate approach for certain hardware constraints.\n\nFor higher fidelity readings, you will need a ADC pin.\nThis will convert analog voltages to a range of digital values. \nEvery ADC has a different bit resolution - for example, the ESP32 has ADCs with a bit resolution of 12 bits - allowing for readings from 0 to 4095.\n\nDigital input pins can also be used to read complex values beyond 0 and 1, but then require the use of a communication protocol.\nThe digital GPIO pin will still only receive 0s and 1s, but the sequence and timing of these bits can encode more complex information.\nMore information on communication protocols is [available in this text](../../1_Preliminaries/3_Communication.md), as well as online.\n\n---\n\n## Application Domains \n\n---\n\n### Digital Instruments\n\nInteractive devices can be used to assist artists in performance.\nAs before, our system will produce some output, but now we have a physical input that is visible to the viewer.\nThis is a key component of digital instrument design, and interactive devices more generally.\nThe ability of the viewer to appreciate the input mechanism allows the viewer to construct a model of virtuosity of the performer.\nIn the same why the physical challenge of playing a violin communicates expertise, by seeing how the artist interacts with the interactive device, a viewer can appreciate the expertise the artist needed to develop in order to create the performance.\n\nThis issue of physically visible input also brings us to the problematic distinction between artist and device designer/developer.\nShould we distinguish between these two roles?\nAnd if so, who's responsibility/right is it to make this distinction?\nBy designing a creative embedded system, is the device designer now, by virtue of working in an artistic realm, an artist?\nIn the case of a team of an artist and device designer, is it possible to assign absolute roles, and is there any value to such an assignment\nIs the concept of an identity as an artist or embedded system engineer inherently exclusionary (the purpose is to give value to membership of an group) or inclusionary (the purpose is to build a community around those who self-identify)?\n\nThis discussion is complicated by the concept of the observer.\nAn observer typically has limited time with the device, and approaches the device without prior instruction on its use.\nThis can lead no interaction behavior that pushes boundaries on the intended functionality - potentially leading to novel uses, and/or system failures.\nA device designed for the causal observer will implement a different design strategy than a device designed for the expert user.\n\nWe can make a clear distinction between casual observers and expert users of a device, especially in the design phase.\nA device designed for an expert user may have more functionality that is less accessible, with the understanding that it is the expert performer's responsibility to gain mastery of the functionality.\nIt may also be the responsibility of the expert, like the casual observer, to push the boundaries of the device's interaction modes.\n\nThe video above shows an example of an expert device - the Mimo glove, as performed by Imogen Heap.\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/6btFObRRD9k?start=944\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\n#### Multiplayer instruments\n\nDigital instruments can also be designed for use with multiple performers.\nIn the video below, the Data Duo synth demonstrates how the physical design of the instrument lends itself to multiplayer input.\n\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/qgyAASXtZj8\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\n## Enclosure Design\n\nBuilding the hardware components is only half the task in an embedded systes - you must also consider how your device will be packaged.\nAn enclosure for a device can make the difference between a “fragile experiment” and a “functional product”.\nAn enclosure defines the physical form factor of the device.\nIn designing a physical form factor for a device, the artist implicitly suggests an interface and mode of interaction to users.\n\nThe idea of an implicit mode of interaction has been explored in the computer graphics community. \nIn the image below from Lau et al. *Tactile Mesh Saliency* in SIGGRAPH 2016, machine learning is used to predict how users will interact with a device.\nFollow up work from Brahmbhatt et al. *ContactDB: Analyzing and Predicting Grasp Contact via Thermal Imaging* in CVPR 2019 continued to explore this idea as well.\nThis prediction can then be shown to the artists during the device design stage to more quickly iterate designs.\n\nWhen time allows, iterating designs with user feedback in the design loop produces devices with more usable interfaces.\nIn so called *pilot studies* of your device, it is important to observe - not guide. \nThis will allow you to see the natural interaction modalities user tend to explore on their own.\nTo explore further, Ask non-leading questions to users.\nFor example, ask \"what did you expect this button to do?\" rather than \"did you expect this button to do X?\"\nAsk \"what do you want the device could do?\" rather than \"would you want the device to do X?\"\n\nA device may have multiple enclosures, either connected via wires or (in Module 4) wirelessly.\nAdditionally, when we explore networked systems in Module 6, a system may comprise multiple devices.\nIn this case, complementary form factors may help increase usability.\n\nIn designing an enclosure, it is also important to take into account where the enclosure will be open.\nDepending on the hardware being used and intensity of the computation, it may be necessary to leave openings for ventilation.\nWhen a system is completely sealed, heat transfer may be a problem - potentially overheating your components under periods of prolonged use.\nSome openings may also be left for wired connections, for example power or 3.5mm audio cables.\n\n### Documenting enclosures\n\nIn the presentation of a device, an enclosure is one of the most immediately obvious features to the viewers.\nAs the vast majority of your audience is likely to only observe your device through the documentation you produce, it is worth investing more time in cleaning up documentation as much as possible.\nIn documenting the enclosure, basic best practices for photography should be observed.\nAs an example, photographing or video recordings should be (as appropriate) against solid color backgrounds to better call attention to the device itself.\nWhen documenting the device in action, extra wires should be hidden as much as possible.\nMore information on best practices that apply well to this domain can be found in *Picturing Science and Engineering* by Felice C. Frankel.\n\n---\n\n\n## Assignment \n\nDue Feb 15\n\nThe second assignment is to place yourself in the position of an interactive instrument device designer. You are asked to build a “performable” device. Performable is broadly interpreted here and may include devices such as digital instruments, or game consoles. \n\nYou are placed under the following design constraints: \n\n- You are allocated three types of sensors: a momentary button (x2), a DPST switch (x1), and an analog joystick (x1). You must utilize exactly these four components.\n- Must demonstrate three different \"modes\" of operation. A mode of operation is defined as: a state of the system that alters the effect an input action has on the output. For example, in one mode of operation, a button press will trigger a sound, and in a second mode of operation, the same button press will trigger a visual effect. While in the context of generative art, the continuously evolving system state might induce new modes of operations, for this task the modes of operation must be directly user controller. You can think of modes of operations as a state machine, where each state is a different mode.\n- The device must have an enclosure or enclosures.\n- You must use the ESP32 and the Raspberry Pi.\n- You may use as many LEDs as you like. Additionally, you may use audio digital out (from the Pi) or visual (monitor, mini-screen, projector etc) digital out. \n\nTwo primary technical challenges you must overcome are 1) aggregating the sensor readings from both the Pi and the ESP32  2) manage switching between three states with only one binary switch\n\n### Deliverables:\n\nSubmit a link to your blog post on the course blog. That post should contain:\n\n(10 pts total - see below for breakdown) A link to your git repository with a program that runs on a Raspberry Pi as well as the program running on the ESP32. Your system must meet the following criteria:\n\n- (3 pts) Uses exactly the sensors listed above and demonstrates three modes of operation.\n- (2 pts) Utilize the joystick as analog input.\n- (2 pts) Have an enclosure.\n- (2 pts) Is a performable device.\n- (1 pts) Is in the spirit of the class as broadly interpreted by the instructors. Art is subjective, we want you to get comfortable with this ethos. \n\n(10 pts) As always, the standard deliverables. The post should detail the creative vision of the device you have created. You should detail the technical challenges that you faced during the implementation of the device. Include a video of your device in action. Specifically, you must make a recording of you peers performing on your device, for whatever “perform” means for your device. The video should be recording following the best practice procedure for documentation covered in class.",
      "html": "<h1>Interactive Devices</h1>\n<h2>System Configuration</h2>\n<pre><code>          |------|\n          |  mp  | ---&gt; D_out\n          |------|  \n             ^\n             |\n             |\n             | \n             v\n          |------|\nA_in ---&gt; |  mc  |\n          |------|  \n</code></pre>\n<p>In this module, we extend our previous configuration with analog input.\nWe use the term analog input to capture the class of circuit designs that allow us to pull data from the real (analog) world into the digital world.\nAccordingly, we now include A<sub>in</sub> (analog input) in our system configuration specification.</p>\n<h3>Digital vs Analog Input</h3>\n<p>In addition to analog input, we now also include digital input in our systems.\nIn fact, the distinction between digital and analog input is somewhat arbitrary.\nDigital input is voltage reduced to either high (1) or low (0) value on your computation device.\nIn practice, a high digital signal value is an analog signal value with a value near the top of the voltage range, while a low digital is a voltage signal near the bottom of the range.\nSimilarly, analog input is voltage reduced to the space of the number of bits your computation device support on its digital pins.</p>\n<p>From a hardware perspective, the Raspberry Pi includes a set of GPIO pins that allow you to interface with circuits.\nWithout an ADC (analog to digital converter, for input) or DAC (digital to analog converter, for output), you cannot use the Raspberry Pi for analog signals - all signals are digital (0 or 1).\nThis is not to say that you cannot use analog input devices, just that they will be reduced to digital inputs.\nAt times this will be the most appropriate approach for certain hardware constraints.</p>\n<p>For higher fidelity readings, you will need a ADC pin.\nThis will convert analog voltages to a range of digital values.\nEvery ADC has a different bit resolution - for example, the ESP32 has ADCs with a bit resolution of 12 bits - allowing for readings from 0 to 4095.</p>\n<p>Digital input pins can also be used to read complex values beyond 0 and 1, but then require the use of a communication protocol.\nThe digital GPIO pin will still only receive 0s and 1s, but the sequence and timing of these bits can encode more complex information.\nMore information on communication protocols is <a href=\"../../1_Preliminaries/3_Communication.html\">available in this text</a>, as well as online.</p>\n<hr>\n<h2>Application Domains</h2>\n<hr>\n<h3>Digital Instruments</h3>\n<p>Interactive devices can be used to assist artists in performance.\nAs before, our system will produce some output, but now we have a physical input that is visible to the viewer.\nThis is a key component of digital instrument design, and interactive devices more generally.\nThe ability of the viewer to appreciate the input mechanism allows the viewer to construct a model of virtuosity of the performer.\nIn the same why the physical challenge of playing a violin communicates expertise, by seeing how the artist interacts with the interactive device, a viewer can appreciate the expertise the artist needed to develop in order to create the performance.</p>\n<p>This issue of physically visible input also brings us to the problematic distinction between artist and device designer/developer.\nShould we distinguish between these two roles?\nAnd if so, who’s responsibility/right is it to make this distinction?\nBy designing a creative embedded system, is the device designer now, by virtue of working in an artistic realm, an artist?\nIn the case of a team of an artist and device designer, is it possible to assign absolute roles, and is there any value to such an assignment\nIs the concept of an identity as an artist or embedded system engineer inherently exclusionary (the purpose is to give value to membership of an group) or inclusionary (the purpose is to build a community around those who self-identify)?</p>\n<p>This discussion is complicated by the concept of the observer.\nAn observer typically has limited time with the device, and approaches the device without prior instruction on its use.\nThis can lead no interaction behavior that pushes boundaries on the intended functionality - potentially leading to novel uses, and/or system failures.\nA device designed for the causal observer will implement a different design strategy than a device designed for the expert user.</p>\n<p>We can make a clear distinction between casual observers and expert users of a device, especially in the design phase.\nA device designed for an expert user may have more functionality that is less accessible, with the understanding that it is the expert performer’s responsibility to gain mastery of the functionality.\nIt may also be the responsibility of the expert, like the casual observer, to push the boundaries of the device’s interaction modes.</p>\n<p>The video above shows an example of an expert device - the Mimo glove, as performed by Imogen Heap.</p>\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/6btFObRRD9k?start=944\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n<h4>Multiplayer instruments</h4>\n<p>Digital instruments can also be designed for use with multiple performers.\nIn the video below, the Data Duo synth demonstrates how the physical design of the instrument lends itself to multiplayer input.</p>\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/qgyAASXtZj8\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n<h2>Enclosure Design</h2>\n<p>Building the hardware components is only half the task in an embedded systes - you must also consider how your device will be packaged.\nAn enclosure for a device can make the difference between a “fragile experiment” and a “functional product”.\nAn enclosure defines the physical form factor of the device.\nIn designing a physical form factor for a device, the artist implicitly suggests an interface and mode of interaction to users.</p>\n<p>The idea of an implicit mode of interaction has been explored in the computer graphics community.\nIn the image below from Lau et al. <em>Tactile Mesh Saliency</em> in SIGGRAPH 2016, machine learning is used to predict how users will interact with a device.\nFollow up work from Brahmbhatt et al. <em>ContactDB: Analyzing and Predicting Grasp Contact via Thermal Imaging</em> in CVPR 2019 continued to explore this idea as well.\nThis prediction can then be shown to the artists during the device design stage to more quickly iterate designs.</p>\n<p>When time allows, iterating designs with user feedback in the design loop produces devices with more usable interfaces.\nIn so called <em>pilot studies</em> of your device, it is important to observe - not guide.\nThis will allow you to see the natural interaction modalities user tend to explore on their own.\nTo explore further, Ask non-leading questions to users.\nFor example, ask “what did you expect this button to do?” rather than “did you expect this button to do X?”\nAsk “what do you want the device could do?” rather than “would you want the device to do X?”</p>\n<p>A device may have multiple enclosures, either connected via wires or (in Module 4) wirelessly.\nAdditionally, when we explore networked systems in Module 6, a system may comprise multiple devices.\nIn this case, complementary form factors may help increase usability.</p>\n<p>In designing an enclosure, it is also important to take into account where the enclosure will be open.\nDepending on the hardware being used and intensity of the computation, it may be necessary to leave openings for ventilation.\nWhen a system is completely sealed, heat transfer may be a problem - potentially overheating your components under periods of prolonged use.\nSome openings may also be left for wired connections, for example power or 3.5mm audio cables.</p>\n<h3>Documenting enclosures</h3>\n<p>In the presentation of a device, an enclosure is one of the most immediately obvious features to the viewers.\nAs the vast majority of your audience is likely to only observe your device through the documentation you produce, it is worth investing more time in cleaning up documentation as much as possible.\nIn documenting the enclosure, basic best practices for photography should be observed.\nAs an example, photographing or video recordings should be (as appropriate) against solid color backgrounds to better call attention to the device itself.\nWhen documenting the device in action, extra wires should be hidden as much as possible.\nMore information on best practices that apply well to this domain can be found in <em>Picturing Science and Engineering</em> by Felice C. Frankel.</p>\n<hr>\n<h2>Assignment</h2>\n<p>Due Feb 15</p>\n<p>The second assignment is to place yourself in the position of an interactive instrument device designer. You are asked to build a “performable” device. Performable is broadly interpreted here and may include devices such as digital instruments, or game consoles.</p>\n<p>You are placed under the following design constraints:</p>\n<ul>\n<li>You are allocated three types of sensors: a momentary button (x2), a DPST switch (x1), and an analog joystick (x1). You must utilize exactly these four components.</li>\n<li>Must demonstrate three different “modes” of operation. A mode of operation is defined as: a state of the system that alters the effect an input action has on the output. For example, in one mode of operation, a button press will trigger a sound, and in a second mode of operation, the same button press will trigger a visual effect. While in the context of generative art, the continuously evolving system state might induce new modes of operations, for this task the modes of operation must be directly user controller. You can think of modes of operations as a state machine, where each state is a different mode.</li>\n<li>The device must have an enclosure or enclosures.</li>\n<li>You must use the ESP32 and the Raspberry Pi.</li>\n<li>You may use as many LEDs as you like. Additionally, you may use audio digital out (from the Pi) or visual (monitor, mini-screen, projector etc) digital out.</li>\n</ul>\n<p>Two primary technical challenges you must overcome are 1) aggregating the sensor readings from both the Pi and the ESP32  2) manage switching between three states with only one binary switch</p>\n<h3>Deliverables:</h3>\n<p>Submit a link to your blog post on the course blog. That post should contain:</p>\n<p>(10 pts total - see below for breakdown) A link to your git repository with a program that runs on a Raspberry Pi as well as the program running on the ESP32. Your system must meet the following criteria:</p>\n<ul>\n<li>(3 pts) Uses exactly the sensors listed above and demonstrates three modes of operation.</li>\n<li>(2 pts) Utilize the joystick as analog input.</li>\n<li>(2 pts) Have an enclosure.</li>\n<li>(2 pts) Is a performable device.</li>\n<li>(1 pts) Is in the spirit of the class as broadly interpreted by the instructors. Art is subjective, we want you to get comfortable with this ethos.</li>\n</ul>\n<p>(10 pts) As always, the standard deliverables. The post should detail the creative vision of the device you have created. You should detail the technical challenges that you faced during the implementation of the device. Include a video of your device in action. Specifically, you must make a recording of you peers performing on your device, for whatever “perform” means for your device. The video should be recording following the best practice procedure for documentation covered in class.</p>\n",
      "id": 10
    },
    {
      "path": "2_Modules/3_Installation_Art/index.md",
      "url": "2_Modules/3_Installation_Art/index.html",
      "content": "# Installation Art\n\n### System Configuration\n\n\n              |------|\n              |  mp  | ---> D_out\n              |------|  \n                 ^\n                 .\n                 .\n                 . \n                 v\n              |------|\n    A_in ---> |  mc  |\n              |------|  \n\n\nIn this module, we break the physical link between the microcontroller and the server and explore wireless methods of data communication including wi-fi, BlueTooth, BlueTooth Low Energy. Simultaneously, we encounter and incorporate technical and artistic motivations for autonomous creative systems that are potentially transparent to the viewer/listener. Those technologies include power considerations. The aesthetic considerations relate to the acousmatic - the heard but not seen.\n\n## Weighing options\n\nAt first glance, WiFi and Bluetooth accomplish roughly the same goal - to have a wireless link that can send data.\nUpon closer inspection, the technical differences between these technologies has important consequences for the creative design of a system.\nIn asking \"how do we move away from tethered interaction?\" we implicitly ask, \"what is the benefit of going wireless?\" and \"what are artistically motivated reasons for communicating wirelessly?\"\nIn looking at WiFi, Bluetooth and BLE, we need to consider constraints such as power, possible network topologies, device support, and bandwidth.\n\nIn terms of power, we generally see a tradeoff where more power allows for greater range, or greater bandwidth.\nIn the case of WiFi, the most expensive communication method of the three, we achieve greater range.\nWhen comparing Bluetooth and Bluetooth Low Energy, we see a larger difference in general between the data transfer rate than the range.\nIn all cases, devices may run at different power levels, so giving an absolute range is not possible for each technology.\nBoth bandwidth and range will also depend on environmental interference, such as other networks, or physical obstacles in the room that do not let a signal pass through.\nAs an example, European houses (often made of thick stone walls) have more trouble propagating a WiFi signal than within the smaller wooden/drywall walls of American style houses.\n\nPower will also influence form factor.\nBy going wireless, your device may be untethered, but then must supply its own power.\nTo do this will most often require a battery (usually LiPo - see appendix for more).\nSome key considerations for choosing a battery will be cost and size (both physical form and total milliAmp hours (mAh)).\nBefore choosing a battery size, think about the use case of your device - how long does it need to run for, can the battery be switched out during a performance, how long is such a performance, how intense is the computation for this application?\nYou should be able to find rough estimates on the power consumption cost of your hardware in various modes of operation (eg. computation vs WiFi broadcast).\n\nAgain thinking of the actual use case of the device, consider what network topologies are most conducive to your artistic vision.\nWhile we address more complex topologies in Module 6 on Distributed Systems, even with two devices, there are considerations to make.\nFor example, when using WiFi, most devices can only connect to one network at a time.\nIf your device requires users to connect (for example, with their phone) to your local WiFi network, that will force users off of any other WiFi network.\nEven within WiFi, the ESP32 can work as both an Access Point (also called a hotspot), but also as a Station (a device connecting to an existing network).\nWhen possible, using the ESP32 as an Access Point is more reliable, as you do not need to rely on any configuration of an existing network.\n\nDevice support is also an issue.\nFor example, if interfacing with mobile phones, Bluetooth only has native support.\nThat is, to communicate with phones over Bluetooth, you will need to write native apps.\nIn contrast, a WiFi network can allow users to open their browser and access interfaces you build and host on your device.\nIn the case of the ESP32, this might include a website that displays a button allowing users to turn an LED on and off (see appendix for activity).\n\nWhen sending messages over a network, you must also consider the protocol and message format.\nFor most embedded systems UDP (faster, easier to set up, but may drop some packets) is the best solutions, as sensor values are continuously streamed from a device.\nAs always, context will dictate whether, TCP or UDP is the right choice.\nYou may consider packaging messages as OSC, when the language support is available.\n\n---\n\n## Module 3 Project: Cutting the cord\n\nDue Monday March 1\n\n---\n\n### Aesthetic Challenges:\n\nThe project for this module challenges you artistically in two ways. \n\nFirst, you must contemplate and ultimately resolve the effect that removing the traditional or expected 1:1, action/reaction/, cause/consequence or process/product pairs that typical installation art has on the viewer or listener. We will refer to this as the \"unseen effect\". To begin to address this you will have to define the high/low latency components of your system, both technically and conceptually. \n\nSecond, to reinforce and possibly assist you in answering the above question, you will be required to both conceive of and design your system combinatorially. That is, you must continue to develop new, creative pathways while incorporating your previous experiences through the reuse of components and algorithms possibly in new, transformed ways.\n\n### Technical Challenges:\n\nTechnically, your project must be designed to take advantage of the new system configuration, specifically 1) wireless communications and 2) battery-powered potential. Previous sensors are in play, as are any other sensors available in your kit.\n\nYou also have a battery (3.7V 650 mAh - provided to those on campus) to allow for a true \"wireless\" experience.\n\nDesign Prompts:\n\nIf your project is mobile, who will move it and how? How can your (or your classmates) Module 2 project inform your design decisions this time around?\n\nYour system components may be visible or masked, a plexiglass homage to form as function or a black box -- impenetrable and imperceivable. \n\nIf your project is immobile, but untethered, might its physical form hint at this reality? \n\nComponent Quick Info:\n\nPiezoelectric sensors detect vibration -- that means that, provided the sensor is mounted tightly to an object, touching the object will result in voltage from the sensor. Useful to detect touch and motion (if the motion results in vibration, such as rolling on wheels across gravel...)\n\nAmbient light sensors (photoresistors) are good (sensitive) and dirt cheap. Useful for detecting... light. But if light is stationary and system is mobile, can be used to detect motion/orientation of the system itself.\n\n\n### Code Deliverables\n\n(10 pts) A link to your git repository with a program that runs on the ESP32 (possibly also the Raspberry Pi) that wirelessly communicates sensor data to a client computer for visualization or sonification. If you do not submit code you will not be awarded any points for the code deliverables section. The program must meet the following criteria:\n\n-\t(5 pts) require wirelessness (convincingly) by addressing, in physical design and realization, the issue of \"unseen effect.\"\n-\t(4 pts) Successfully combine earlier system configurations through use/reuse of sensors, config code, and physical design principles.\n-\t(1 pts) Is in the spirit of the class as broadly interpreted by the instructors. Art is subjective, we want you to get comfortable with this ethos.\n\n\n### Standard Documentation Deliverables:\n\n10 points\n\nIn addition to the project specific deliverables lists above, you must also meet the following “standard documentation deliverables”. Throughout this course, we will ask you to document your work in order to slowly build a portfolio of your projects. Going forward, these types of standard documentation deliverables can be assumed to be required for all assignments unless specified otherwise.\n\nA blog post\n\nUsing the CoursePress site available through Canvas, make a blog post describing your art. The post should give an overview of your artistic vision. In particular for this assignment, you should address how you have specialized your generative art to the space. What creative decisions did you work lead you to, and which decisions did you take? How were your decisions motivated by your large creative vision for this project. In the same vein, also address any technical issues you encountered in your work. Particularly focus on issues that other artists may encounter when developing a generative art display for this space.\n\nA README\n\nOn your github repo add a readme that contains a short description and key information on reproducibility/installation/usage. \nThis key information should be sufficient for a knowledge third party, outside the class, to replicate your design.\nThis readme can/should be a subset of the material used in your CoursePress blog post\n\nA video of your art\n\nInclude in the README a link to your video. The video can be a simple video shot on your phone - the only goal is to have a record of your art in action. You can host the video wherever you like as long as the hosting platform supports in-browser playback (e.g. YouTube, Vimeo).\n\n\n",
      "html": "<h1>Installation Art</h1>\n<h3>System Configuration</h3>\n<pre><code>          |------|\n          |  mp  | ---&gt; D_out\n          |------|  \n             ^\n             .\n             .\n             . \n             v\n          |------|\nA_in ---&gt; |  mc  |\n          |------|  \n</code></pre>\n<p>In this module, we break the physical link between the microcontroller and the server and explore wireless methods of data communication including wi-fi, BlueTooth, BlueTooth Low Energy. Simultaneously, we encounter and incorporate technical and artistic motivations for autonomous creative systems that are potentially transparent to the viewer/listener. Those technologies include power considerations. The aesthetic considerations relate to the acousmatic - the heard but not seen.</p>\n<h2>Weighing options</h2>\n<p>At first glance, WiFi and Bluetooth accomplish roughly the same goal - to have a wireless link that can send data.\nUpon closer inspection, the technical differences between these technologies has important consequences for the creative design of a system.\nIn asking “how do we move away from tethered interaction?” we implicitly ask, “what is the benefit of going wireless?” and “what are artistically motivated reasons for communicating wirelessly?”\nIn looking at WiFi, Bluetooth and BLE, we need to consider constraints such as power, possible network topologies, device support, and bandwidth.</p>\n<p>In terms of power, we generally see a tradeoff where more power allows for greater range, or greater bandwidth.\nIn the case of WiFi, the most expensive communication method of the three, we achieve greater range.\nWhen comparing Bluetooth and Bluetooth Low Energy, we see a larger difference in general between the data transfer rate than the range.\nIn all cases, devices may run at different power levels, so giving an absolute range is not possible for each technology.\nBoth bandwidth and range will also depend on environmental interference, such as other networks, or physical obstacles in the room that do not let a signal pass through.\nAs an example, European houses (often made of thick stone walls) have more trouble propagating a WiFi signal than within the smaller wooden/drywall walls of American style houses.</p>\n<p>Power will also influence form factor.\nBy going wireless, your device may be untethered, but then must supply its own power.\nTo do this will most often require a battery (usually LiPo - see appendix for more).\nSome key considerations for choosing a battery will be cost and size (both physical form and total milliAmp hours (mAh)).\nBefore choosing a battery size, think about the use case of your device - how long does it need to run for, can the battery be switched out during a performance, how long is such a performance, how intense is the computation for this application?\nYou should be able to find rough estimates on the power consumption cost of your hardware in various modes of operation (eg. computation vs WiFi broadcast).</p>\n<p>Again thinking of the actual use case of the device, consider what network topologies are most conducive to your artistic vision.\nWhile we address more complex topologies in Module 6 on Distributed Systems, even with two devices, there are considerations to make.\nFor example, when using WiFi, most devices can only connect to one network at a time.\nIf your device requires users to connect (for example, with their phone) to your local WiFi network, that will force users off of any other WiFi network.\nEven within WiFi, the ESP32 can work as both an Access Point (also called a hotspot), but also as a Station (a device connecting to an existing network).\nWhen possible, using the ESP32 as an Access Point is more reliable, as you do not need to rely on any configuration of an existing network.</p>\n<p>Device support is also an issue.\nFor example, if interfacing with mobile phones, Bluetooth only has native support.\nThat is, to communicate with phones over Bluetooth, you will need to write native apps.\nIn contrast, a WiFi network can allow users to open their browser and access interfaces you build and host on your device.\nIn the case of the ESP32, this might include a website that displays a button allowing users to turn an LED on and off (see appendix for activity).</p>\n<p>When sending messages over a network, you must also consider the protocol and message format.\nFor most embedded systems UDP (faster, easier to set up, but may drop some packets) is the best solutions, as sensor values are continuously streamed from a device.\nAs always, context will dictate whether, TCP or UDP is the right choice.\nYou may consider packaging messages as OSC, when the language support is available.</p>\n<hr>\n<h2>Module 3 Project: Cutting the cord</h2>\n<p>Due Monday March 1</p>\n<hr>\n<h3>Aesthetic Challenges:</h3>\n<p>The project for this module challenges you artistically in two ways.</p>\n<p>First, you must contemplate and ultimately resolve the effect that removing the traditional or expected 1:1, action/reaction/, cause/consequence or process/product pairs that typical installation art has on the viewer or listener. We will refer to this as the “unseen effect”. To begin to address this you will have to define the high/low latency components of your system, both technically and conceptually.</p>\n<p>Second, to reinforce and possibly assist you in answering the above question, you will be required to both conceive of and design your system combinatorially. That is, you must continue to develop new, creative pathways while incorporating your previous experiences through the reuse of components and algorithms possibly in new, transformed ways.</p>\n<h3>Technical Challenges:</h3>\n<p>Technically, your project must be designed to take advantage of the new system configuration, specifically 1) wireless communications and 2) battery-powered potential. Previous sensors are in play, as are any other sensors available in your kit.</p>\n<p>You also have a battery (3.7V 650 mAh - provided to those on campus) to allow for a true “wireless” experience.</p>\n<p>Design Prompts:</p>\n<p>If your project is mobile, who will move it and how? How can your (or your classmates) Module 2 project inform your design decisions this time around?</p>\n<p>Your system components may be visible or masked, a plexiglass homage to form as function or a black box – impenetrable and imperceivable.</p>\n<p>If your project is immobile, but untethered, might its physical form hint at this reality?</p>\n<p>Component Quick Info:</p>\n<p>Piezoelectric sensors detect vibration – that means that, provided the sensor is mounted tightly to an object, touching the object will result in voltage from the sensor. Useful to detect touch and motion (if the motion results in vibration, such as rolling on wheels across gravel…)</p>\n<p>Ambient light sensors (photoresistors) are good (sensitive) and dirt cheap. Useful for detecting… light. But if light is stationary and system is mobile, can be used to detect motion/orientation of the system itself.</p>\n<h3>Code Deliverables</h3>\n<p>(10 pts) A link to your git repository with a program that runs on the ESP32 (possibly also the Raspberry Pi) that wirelessly communicates sensor data to a client computer for visualization or sonification. If you do not submit code you will not be awarded any points for the code deliverables section. The program must meet the following criteria:</p>\n<ul>\n<li>(5 pts) require wirelessness (convincingly) by addressing, in physical design and realization, the issue of “unseen effect.”</li>\n<li>(4 pts) Successfully combine earlier system configurations through use/reuse of sensors, config code, and physical design principles.</li>\n<li>(1 pts) Is in the spirit of the class as broadly interpreted by the instructors. Art is subjective, we want you to get comfortable with this ethos.</li>\n</ul>\n<h3>Standard Documentation Deliverables:</h3>\n<p>10 points</p>\n<p>In addition to the project specific deliverables lists above, you must also meet the following “standard documentation deliverables”. Throughout this course, we will ask you to document your work in order to slowly build a portfolio of your projects. Going forward, these types of standard documentation deliverables can be assumed to be required for all assignments unless specified otherwise.</p>\n<p>A blog post</p>\n<p>Using the CoursePress site available through Canvas, make a blog post describing your art. The post should give an overview of your artistic vision. In particular for this assignment, you should address how you have specialized your generative art to the space. What creative decisions did you work lead you to, and which decisions did you take? How were your decisions motivated by your large creative vision for this project. In the same vein, also address any technical issues you encountered in your work. Particularly focus on issues that other artists may encounter when developing a generative art display for this space.</p>\n<p>A README</p>\n<p>On your github repo add a readme that contains a short description and key information on reproducibility/installation/usage.\nThis key information should be sufficient for a knowledge third party, outside the class, to replicate your design.\nThis readme can/should be a subset of the material used in your CoursePress blog post</p>\n<p>A video of your art</p>\n<p>Include in the README a link to your video. The video can be a simple video shot on your phone - the only goal is to have a record of your art in action. You can host the video wherever you like as long as the hosting platform supports in-browser playback (e.g. YouTube, Vimeo).</p>\n",
      "id": 11
    },
    {
      "path": "2_Modules/4_Kinetic_Scuplture/index.md",
      "url": "2_Modules/4_Kinetic_Scuplture/index.html",
      "content": "# Kinetic Sculpture\n\n### System Configuration (Module 4)\n\n              |------|\n              |  mp  |\n              |------|  \n                 ^\n                 .\n                 .\n                 .\n                 v\n              |------|\n              |  mc  | ---> A_out\n              |------|  \n\n\nModule 4 introduces actuators (motors, LEDs) with A<sub>out</sub> to allow our system to take action in the physical world. \nSimilar to Module 1 on [Generative Art](../1_Generative_Art/index.md), this system configuration takes no inputs.\nHere we focus on generative systems that produce analog outputs, rather than digital outputs.\nWe can still optionally include a connection a microprocessor for digital output.\n\n\n# Interactive Kinetic Art\n\n### System Configuration \n\n              |------|\n              |  mp  | ---> D_out\n              |------|  \n                 ^\n                 .\n                 .\n                 . \n                 v\n              |------|\n    A_in ---> |  mc  | ---> A_out\n              |------|  \n\nIn module 5 we combine all previous system configurations to build installations that can take user input A<sub>in</sub> and actuate A<sub>out</sub> accordingly. This system configuration is utilized for purposes of augmented experience design. \nIn this chapter, we combine modules 4 and 5.\n\n## Basic Philosophy of Kinetic Art\n\nKinetic art in the context of creative embedded systems bridges the gap between the \"cyber\" and the \"physical\".\nThis appeals to our more visceral human instincts to examine motion.\nWith such systems, we can visualize bits being flipped or augment motion with computation to create complex patterns that would de difficult to implement in a purely physical system.\n\nTo first explore the basics of kinetic sculpture, view the video below which illustrates some core ideas behind the practice - outside the realm of embedded systems.\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/nxdcj2tLQGE\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\n----\n\n\n# Examples\n\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/1_lBHkdEYJk\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\n\n## Module 4: Actuators\n\nDue March 26th - no late work please!\n\nThis assignment requires you actuate the physical world - you may choose one of two configurations for this project - either 1) kinetic sculpture or 2) Interactive kinetic art. The difference amounts to whether or not you choose to integrate input from the user into your work. You are provided in your kit with the following hardware.\n\n- 28BYJ-48 Step Motor\n- SG90 Tower Pro MicroServo\n- DC Brush motor\n\nYou may create a team of up to three group members. Your group may not have overlapping members from the previous assignments. When working in a group, your hardware resources are pooled. Your device must adhere to the following specification.\nYou must use a minimum of (2*number of group members) motors and may use as many motors as you have available to your group.\nThe device must have an enclosure or enclosures.\n\n\nYour device must utilize an API (to be provided) that will allow you to start the device in motion.\nMore details to come on the API usage in class.\nThe device must be safe to operate in intervals of at least 30 seconds multiple times in a row.\n\nIf you are interested, you are invited to display your work in Milstein on the 5th floor for the time period of March 27-28.\n\n##### Deliverables\n\n40 pts\n\n- (10 pts) A link to your git repository with a program that runs on the ESP32 or the Raspberry Pi to control your device. The program must meet the following criteria:\n- (10 pts) Utilizes your actuators\n- (8 pts) Successfully combine earlier system configurations through use/reuse of sensors, config code, and physical design principles.\n- (2 pts) Is in the spirit of the class as broadly interpreted by the instructors. Art is subjective, we want you to get comfortable with this ethos.\n- (10 pts) Quality and thoroughness of documentation and standard deliverables.\n\nA video of your device in action.\n\nAs always, the standard deliverables. If working in a group, you may all submit the same code repository and a single blog post. The post should detail, in your own words, the creative vision of the device you have created. You should detail the technical challenges you specifically (as opposed to other group members) faced during the implementation of the device.\n\n\n",
      "html": "<h1>Kinetic Sculpture</h1>\n<h3>System Configuration (Module 4)</h3>\n<pre><code>          |------|\n          |  mp  |\n          |------|  \n             ^\n             .\n             .\n             .\n             v\n          |------|\n          |  mc  | ---&gt; A_out\n          |------|  \n</code></pre>\n<p>Module 4 introduces actuators (motors, LEDs) with A<sub>out</sub> to allow our system to take action in the physical world.\nSimilar to Module 1 on <a href=\"../1_Generative_Art/index.html\">Generative Art</a>, this system configuration takes no inputs.\nHere we focus on generative systems that produce analog outputs, rather than digital outputs.\nWe can still optionally include a connection a microprocessor for digital output.</p>\n<h1>Interactive Kinetic Art</h1>\n<h3>System Configuration</h3>\n<pre><code>          |------|\n          |  mp  | ---&gt; D_out\n          |------|  \n             ^\n             .\n             .\n             . \n             v\n          |------|\nA_in ---&gt; |  mc  | ---&gt; A_out\n          |------|  \n</code></pre>\n<p>In module 5 we combine all previous system configurations to build installations that can take user input A<sub>in</sub> and actuate A<sub>out</sub> accordingly. This system configuration is utilized for purposes of augmented experience design.\nIn this chapter, we combine modules 4 and 5.</p>\n<h2>Basic Philosophy of Kinetic Art</h2>\n<p>Kinetic art in the context of creative embedded systems bridges the gap between the “cyber” and the “physical”.\nThis appeals to our more visceral human instincts to examine motion.\nWith such systems, we can visualize bits being flipped or augment motion with computation to create complex patterns that would de difficult to implement in a purely physical system.</p>\n<p>To first explore the basics of kinetic sculpture, view the video below which illustrates some core ideas behind the practice - outside the realm of embedded systems.</p>\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/nxdcj2tLQGE\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n<hr>\n<h1>Examples</h1>\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/1_lBHkdEYJk\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n<h2>Module 4: Actuators</h2>\n<p>Due March 26th - no late work please!</p>\n<p>This assignment requires you actuate the physical world - you may choose one of two configurations for this project - either 1) kinetic sculpture or 2) Interactive kinetic art. The difference amounts to whether or not you choose to integrate input from the user into your work. You are provided in your kit with the following hardware.</p>\n<ul>\n<li>28BYJ-48 Step Motor</li>\n<li>SG90 Tower Pro MicroServo</li>\n<li>DC Brush motor</li>\n</ul>\n<p>You may create a team of up to three group members. Your group may not have overlapping members from the previous assignments. When working in a group, your hardware resources are pooled. Your device must adhere to the following specification.\nYou must use a minimum of (2*number of group members) motors and may use as many motors as you have available to your group.\nThe device must have an enclosure or enclosures.</p>\n<p>Your device must utilize an API (to be provided) that will allow you to start the device in motion.\nMore details to come on the API usage in class.\nThe device must be safe to operate in intervals of at least 30 seconds multiple times in a row.</p>\n<p>If you are interested, you are invited to display your work in Milstein on the 5th floor for the time period of March 27-28.</p>\n<h5>Deliverables</h5>\n<p>40 pts</p>\n<ul>\n<li>(10 pts) A link to your git repository with a program that runs on the ESP32 or the Raspberry Pi to control your device. The program must meet the following criteria:</li>\n<li>(10 pts) Utilizes your actuators</li>\n<li>(8 pts) Successfully combine earlier system configurations through use/reuse of sensors, config code, and physical design principles.</li>\n<li>(2 pts) Is in the spirit of the class as broadly interpreted by the instructors. Art is subjective, we want you to get comfortable with this ethos.</li>\n<li>(10 pts) Quality and thoroughness of documentation and standard deliverables.</li>\n</ul>\n<p>A video of your device in action.</p>\n<p>As always, the standard deliverables. If working in a group, you may all submit the same code repository and a single blog post. The post should detail, in your own words, the creative vision of the device you have created. You should detail the technical challenges you specifically (as opposed to other group members) faced during the implementation of the device.</p>\n",
      "id": 12
    },
    {
      "path": "2_Modules/7_Final_Project/index.md",
      "url": "2_Modules/7_Final_Project/index.html",
      "content": "# Final Project\n\n### System Configuration\n\nChoose your own adventure! Any system configuration goes for the final project.\n\n              |------|\n    D_out---> |  mp  | ---> D_out\n              |------|  \n                 ^ ^\n                 . |\n                 . |\n                 . | \n                 v v\n              |------|       |------|\n    A_in ---> |  mc  | <---> |  mc  |\n              |------|       |------|\n\nThe final module of the class is an open-ended exploration of system configurations we have explored in class. Students are required to use a minimum set of sensors, actuators, and digital outputs in their own designs. \n\n---\n\n## Final Project\n\nDue Friday April 23 (absolutely no late work can be accepted per college policy!)\n\nProposal due Sunday April 4\n\n---\n\nThe final project is a chance for you to synthesize the topics we have covered so far in class (and the topics we will soon cover). This project may be a group project of groups of up to three. If this is a group project, at least two ESP32s must be used and communicate directly with each other using wifi, bluetooth, or ESP-NOW (the ESP32 specific communication protocol - covered briefly in class 13).\n\n### Proposal\n\n(5 points) A short plaintext description of your idea submitted to CourseWorks. Include in your proposal:\n\n- a high level description of the idea (1 paragraphs)\n- a plan for the major checkpoints (with dates) you need to hit in order to accomplish that idea (a list including at least 4 checkpoints)\n- the new piece of hardware or software you will include (1 paragraph - why this piece and what resources will you utilize to learn how this work)\n\n### Code Deliverables\n\n(25 pts) A link to your git repository with a program that runs on at least one ESP32 (possibly also the Raspberry Pi).\n\nThe formal requirements are even more open ended than previous modules.\n\nYou must utilize some technique we have not yet covered in class. \nThat could take the form of a new piece of hardware from your kit, a new piece of hardware you own/buy on your own, or a new software technique.\nNew software techniques might include utilizing the multicore processor (as covered in class 27 on March 24), communicating between multiple ESP32s, or using the ESP-IDF development toolchain (to be covered in class 30 on March 31).\n\nBeyond this, simply follow the same documentation guidelines we have established in prior assignments.\n\n### Standard Documentation Deliverables:\n\n20 points\n\nIn addition to the project specific deliverables lists above, you must also meet the following “standard documentation deliverables”. Throughout this course, we will ask you to document your work in order to slowly build a portfolio of your projects. Going forward, these types of standard documentation deliverables can be assumed to be required for all assignments unless specified otherwise.\n\nA blog post\n\nUsing the CoursePress site available through Canvas, make a blog post describing your art. The post should give an overview of your artistic vision. In particular for this assignment, you should address how you have specialized your generative art to the space. What creative decisions did you work lead you to, and which decisions did you take? How were your decisions motivated by your large creative vision for this project. In the same vein, also address any technical issues you encountered in your work. Particularly focus on issues that other artists may encounter when developing a generative art display for this space.\n\nA README\n\nOn your github repo add a readme that contains a short description and key information on reproducibility/installation/usage. \nThis key information should be sufficient for a knowledge third party, outside the class, to replicate your design.\nThis readme can/should be a subset of the material used in your CoursePress blog post.\n\nA video of your art\n\nInclude in the README a link to your video. The video can be a simple video shot on your phone - the only goal is to have a record of your art in action. You can host the video wherever you like as long as the hosting platform supports in-browser playback (e.g. YouTube, Vimeo).\n\n",
      "html": "<h1>Final Project</h1>\n<h3>System Configuration</h3>\n<p>Choose your own adventure! Any system configuration goes for the final project.</p>\n<pre><code>          |------|\nD_out---&gt; |  mp  | ---&gt; D_out\n          |------|  \n             ^ ^\n             . |\n             . |\n             . | \n             v v\n          |------|       |------|\nA_in ---&gt; |  mc  | &lt;---&gt; |  mc  |\n          |------|       |------|\n</code></pre>\n<p>The final module of the class is an open-ended exploration of system configurations we have explored in class. Students are required to use a minimum set of sensors, actuators, and digital outputs in their own designs.</p>\n<hr>\n<h2>Final Project</h2>\n<p>Due Friday April 23 (absolutely no late work can be accepted per college policy!)</p>\n<p>Proposal due Sunday April 4</p>\n<hr>\n<p>The final project is a chance for you to synthesize the topics we have covered so far in class (and the topics we will soon cover). This project may be a group project of groups of up to three. If this is a group project, at least two ESP32s must be used and communicate directly with each other using wifi, bluetooth, or ESP-NOW (the ESP32 specific communication protocol - covered briefly in class 13).</p>\n<h3>Proposal</h3>\n<p>(5 points) A short plaintext description of your idea submitted to CourseWorks. Include in your proposal:</p>\n<ul>\n<li>a high level description of the idea (1 paragraphs)</li>\n<li>a plan for the major checkpoints (with dates) you need to hit in order to accomplish that idea (a list including at least 4 checkpoints)</li>\n<li>the new piece of hardware or software you will include (1 paragraph - why this piece and what resources will you utilize to learn how this work)</li>\n</ul>\n<h3>Code Deliverables</h3>\n<p>(25 pts) A link to your git repository with a program that runs on at least one ESP32 (possibly also the Raspberry Pi).</p>\n<p>The formal requirements are even more open ended than previous modules.</p>\n<p>You must utilize some technique we have not yet covered in class.\nThat could take the form of a new piece of hardware from your kit, a new piece of hardware you own/buy on your own, or a new software technique.\nNew software techniques might include utilizing the multicore processor (as covered in class 27 on March 24), communicating between multiple ESP32s, or using the ESP-IDF development toolchain (to be covered in class 30 on March 31).</p>\n<p>Beyond this, simply follow the same documentation guidelines we have established in prior assignments.</p>\n<h3>Standard Documentation Deliverables:</h3>\n<p>20 points</p>\n<p>In addition to the project specific deliverables lists above, you must also meet the following “standard documentation deliverables”. Throughout this course, we will ask you to document your work in order to slowly build a portfolio of your projects. Going forward, these types of standard documentation deliverables can be assumed to be required for all assignments unless specified otherwise.</p>\n<p>A blog post</p>\n<p>Using the CoursePress site available through Canvas, make a blog post describing your art. The post should give an overview of your artistic vision. In particular for this assignment, you should address how you have specialized your generative art to the space. What creative decisions did you work lead you to, and which decisions did you take? How were your decisions motivated by your large creative vision for this project. In the same vein, also address any technical issues you encountered in your work. Particularly focus on issues that other artists may encounter when developing a generative art display for this space.</p>\n<p>A README</p>\n<p>On your github repo add a readme that contains a short description and key information on reproducibility/installation/usage.\nThis key information should be sufficient for a knowledge third party, outside the class, to replicate your design.\nThis readme can/should be a subset of the material used in your CoursePress blog post.</p>\n<p>A video of your art</p>\n<p>Include in the README a link to your video. The video can be a simple video shot on your phone - the only goal is to have a record of your art in action. You can host the video wherever you like as long as the hosting platform supports in-browser playback (e.g. YouTube, Vimeo).</p>\n",
      "id": 13
    },
    {
      "path": "2_Modules/index.md",
      "url": "2_Modules/index.html",
      "content": "# Introduction\n\nThis chapter introduces a number of embedded systems and creative uses of these systems.\n\nThroughout this chapter, we will refer to two actors - the artist and the audience.\nThese roles are roughly equivalent to what more traditional texts might refer to as the developer and the user.\nWe make the distinction here as, for the purposes of this text, the task of the artist does not necessarily align with the tasks of a developer.\nWhereas a developer will typically create a system and deploy it into the world, the types of systems we will examine here may not follow the same scalable distribution strategy.\n\nAs an example, we investigate the practice of live coding in the [Generative Art](./1_Generative_Art/index.md) module, which is an artistic practice requiring constant code modification from the artist to produce content for the audience.\nIn contrast, a typical system built by the more traditional concept of developer would not require real-time code modification to deliver a service to users.\n\nModules will be released as we come to them in the course.\n\n## System configuration\n\nIn this section, we introduce increasingly complex types of embedded systems. \nIn order to organize the evolution of the complexity of these systems, We introduce a notation to describe _system configurations_.\nA system configuration defines, at a high level, the ''interface'' of the embedded system to the artist and audience.\nEach system configuration describes a ''template'' for an embedded system.\n\nWriting down the system configuration is a kind of documentation - it helps others to reproduce the system or gain a more direct understanding of the system, so that they can adapt ideas to their own systems.\nTo make the definition of a system configuration concrete, we introduce the following notation:\n\nA system configuration is a graph of nodes and edges, expressed as a tuple _(N,E)_. \nThe set of nodes _N_ consist of devices _D_, inputs _I_, outputs _O_, while the edges _E:N1 x N2 x T_ are the connections between two inputs, devices, and outputs _N1_ and _N2_, as well as a label _T_ that denotes the type of the edge (data, power, wired/wireless, etc).\n\nThe set of devices will primarily be composed of the computational units described in [Hardware](../1_Preliminaries/2_Hardware.md) (ie microprocessors, microcontrollers, circuits).\nHowever, the set of devices does not need to be restricted to these devices - including further types of nodes will give a more detailed picture of the systems.\nFor example, including power sources (eg outlets, battery packs) or data sources/sinks (eg databases, web services) in the set of nodes may be necessary for some systems to ensure the system configuration is complete enough to be reproduced.\n\nWe partition the inputs, _I_, and outputs, _O_, into two sets - digital and analog.\nWe consider digital inputs and outputs (D<sub>in</sub> and D<sub>out</sub> respectively), and analog inputs and outputs (A<sub>in</sub> and A<sub>out</sub> respectively).\nWe draw the distinction between digital and analog from the perspective of the device designer.\nFrom the perspective of the viewer, as the viewer processes all media with their physical senses, all inputs and outputs to a system might be considered analog.\nwhile a fair argument, this is not a useful distinction for the purposes of this text.\n\n\nThe set of edges indicate how the various devices of the system are connected.\nGenerally, we take the edges to be undirected, as it will generally be clear from the type _T_ of the wire (data can flow in both directions over a wire, but power is directed).\nHowever, as with devices, these are only guidelines, and we may choose to add more details to a system configuration as appropriate.\nFor example, when pulling a datastream from the internet, it may be clarifying to include a note on the order.\n\nAs a system configuration is simply a labeled graph, for convenience, we will render system configurations as diagrams.\nIn this text, we use the system diagram notation only for documentation - thus we will not need a more rigorous presentation of system configurations.\n",
      "html": "<h1>Introduction</h1>\n<p>This chapter introduces a number of embedded systems and creative uses of these systems.</p>\n<p>Throughout this chapter, we will refer to two actors - the artist and the audience.\nThese roles are roughly equivalent to what more traditional texts might refer to as the developer and the user.\nWe make the distinction here as, for the purposes of this text, the task of the artist does not necessarily align with the tasks of a developer.\nWhereas a developer will typically create a system and deploy it into the world, the types of systems we will examine here may not follow the same scalable distribution strategy.</p>\n<p>As an example, we investigate the practice of live coding in the <a href=\"./1_Generative_Art/index.html\">Generative Art</a> module, which is an artistic practice requiring constant code modification from the artist to produce content for the audience.\nIn contrast, a typical system built by the more traditional concept of developer would not require real-time code modification to deliver a service to users.</p>\n<p>Modules will be released as we come to them in the course.</p>\n<h2>System configuration</h2>\n<p>In this section, we introduce increasingly complex types of embedded systems.\nIn order to organize the evolution of the complexity of these systems, We introduce a notation to describe <em>system configurations</em>.\nA system configuration defines, at a high level, the ‘‘interface’’ of the embedded system to the artist and audience.\nEach system configuration describes a ‘‘template’’ for an embedded system.</p>\n<p>Writing down the system configuration is a kind of documentation - it helps others to reproduce the system or gain a more direct understanding of the system, so that they can adapt ideas to their own systems.\nTo make the definition of a system configuration concrete, we introduce the following notation:</p>\n<p>A system configuration is a graph of nodes and edges, expressed as a tuple <em>(N,E)</em>.\nThe set of nodes <em>N</em> consist of devices <em>D</em>, inputs <em>I</em>, outputs <em>O</em>, while the edges <em>E:N1 x N2 x T</em> are the connections between two inputs, devices, and outputs <em>N1</em> and <em>N2</em>, as well as a label <em>T</em> that denotes the type of the edge (data, power, wired/wireless, etc).</p>\n<p>The set of devices will primarily be composed of the computational units described in <a href=\"../1_Preliminaries/2_Hardware.html\">Hardware</a> (ie microprocessors, microcontrollers, circuits).\nHowever, the set of devices does not need to be restricted to these devices - including further types of nodes will give a more detailed picture of the systems.\nFor example, including power sources (eg outlets, battery packs) or data sources/sinks (eg databases, web services) in the set of nodes may be necessary for some systems to ensure the system configuration is complete enough to be reproduced.</p>\n<p>We partition the inputs, <em>I</em>, and outputs, <em>O</em>, into two sets - digital and analog.\nWe consider digital inputs and outputs (D<sub>in</sub> and D<sub>out</sub> respectively), and analog inputs and outputs (A<sub>in</sub> and A<sub>out</sub> respectively).\nWe draw the distinction between digital and analog from the perspective of the device designer.\nFrom the perspective of the viewer, as the viewer processes all media with their physical senses, all inputs and outputs to a system might be considered analog.\nwhile a fair argument, this is not a useful distinction for the purposes of this text.</p>\n<p>The set of edges indicate how the various devices of the system are connected.\nGenerally, we take the edges to be undirected, as it will generally be clear from the type <em>T</em> of the wire (data can flow in both directions over a wire, but power is directed).\nHowever, as with devices, these are only guidelines, and we may choose to add more details to a system configuration as appropriate.\nFor example, when pulling a datastream from the internet, it may be clarifying to include a note on the order.</p>\n<p>As a system configuration is simply a labeled graph, for convenience, we will render system configurations as diagrams.\nIn this text, we use the system diagram notation only for documentation - thus we will not need a more rigorous presentation of system configurations.</p>\n",
      "id": 14
    },
    {
      "path": "Appendix/How_Low_Can_you_Go.md",
      "url": "Appendix/How_Low_Can_you_Go.html",
      "content": "# How low can you go?\n\nWhen writing code for a microcontroller, there are a number of languages and development tools available to you.\nYou must choose which tools to use.\nYou can choose to go as low level as printing PCBs, or use a high level language like Python or Haskell.\n\n## cmake vs Arduino IDE\n\nWhen writing code for an Arduino, you can use the Arduino IDE and write in a C-like language.\nYou can also write C code directly, and then manually flash the code onto the device.\nFlashing C code onto the device directly is significantly more difficult to get set up than using the Arduino IDE - so why should we go to the extra effort?\n\n### Speed\n\nWhen the Arduino IDE loads a program onto your microcontroller, it includes a number of libraries that are what allow you to write in the simplified Arduino C-like language.\nThis causes a significant impact on boot times and can also induce a slow down at runtime, especially when compared to optimized C code.\nAdditionally, these extra libraries eat up the limited amount of program memory you have on your device.\nWhile for most creative applications, the Arduino IDE will suffice, if you find yourself running into computation or memory bottlenecks, it may be time to investigate settings up a more direct compilation chain using C.\n\n### Space\n\nIn general, high-level languages introduce bloat to handle the abstractions made available to you.\nAt this point, memory is cheap enough, and most compilers are optimized enough that this will not be a meaningful consideration.\n",
      "html": "<h1>How low can you go?</h1>\n<p>When writing code for a microcontroller, there are a number of languages and development tools available to you.\nYou must choose which tools to use.\nYou can choose to go as low level as printing PCBs, or use a high level language like Python or Haskell.</p>\n<h2>cmake vs Arduino IDE</h2>\n<p>When writing code for an Arduino, you can use the Arduino IDE and write in a C-like language.\nYou can also write C code directly, and then manually flash the code onto the device.\nFlashing C code onto the device directly is significantly more difficult to get set up than using the Arduino IDE - so why should we go to the extra effort?</p>\n<h3>Speed</h3>\n<p>When the Arduino IDE loads a program onto your microcontroller, it includes a number of libraries that are what allow you to write in the simplified Arduino C-like language.\nThis causes a significant impact on boot times and can also induce a slow down at runtime, especially when compared to optimized C code.\nAdditionally, these extra libraries eat up the limited amount of program memory you have on your device.\nWhile for most creative applications, the Arduino IDE will suffice, if you find yourself running into computation or memory bottlenecks, it may be time to investigate settings up a more direct compilation chain using C.</p>\n<h3>Space</h3>\n<p>In general, high-level languages introduce bloat to handle the abstractions made available to you.\nAt this point, memory is cheap enough, and most compilers are optimized enough that this will not be a meaningful consideration.</p>\n",
      "id": 15
    },
    {
      "path": "Appendix/Run_on_boot.md",
      "url": "Appendix/Run_on_boot.html",
      "content": "# Running a command on boot\n\nOne key technical skill for building creative embedded devices is to be able to start a program on boot.\nThis will allow non-technical users to plug in your device, and immediately begin consuming your art.\n\nThe basic premise of starting a program on system boot is to insert the program invocation into a script that the OS runs on startup.\nOn the Raspbian Linux distribution (which is likely on your Raspberry Pi), this file is located at `~/.config/lxsession/LXDE-pi/autostart` or `/etc/rc.local`.\n\n## Sonic Pi\n\nIn order to start a Sonic Pi program when your device boots, we can use the Sonic Pi start up file, located at `~/.sonic-pi/init.rb`.\nThis file will be executed every time Sonic Pi is started - by then automatically running Sonic Pi on boot through, for example the startup file list above, you can always start your system on device boot.\n\nAnother approach is to use the `sonic-pi-tool`.\n\n## Processing\n\nAdd a line `/usr/local/bin/processing-java --sketch=/home/pi/sketchbook/sketchname --run` to your startup script file.\n\n# Running a command on device connect\n\nYou may also want to run a command automatically when a device (eg a usb) is plugged in.\nOne way to do this is with udev rules.\nYou can make a new udev rule by adding a file under the `/etc/udev/rules.d/` directory.\n",
      "html": "<h1>Running a command on boot</h1>\n<p>One key technical skill for building creative embedded devices is to be able to start a program on boot.\nThis will allow non-technical users to plug in your device, and immediately begin consuming your art.</p>\n<p>The basic premise of starting a program on system boot is to insert the program invocation into a script that the OS runs on startup.\nOn the Raspbian Linux distribution (which is likely on your Raspberry Pi), this file is located at <code>~/.config/lxsession/LXDE-pi/autostart</code> or <code>/etc/rc.local</code>.</p>\n<h2>Sonic Pi</h2>\n<p>In order to start a Sonic Pi program when your device boots, we can use the Sonic Pi start up file, located at <code>~/.sonic-pi/init.rb</code>.\nThis file will be executed every time Sonic Pi is started - by then automatically running Sonic Pi on boot through, for example the startup file list above, you can always start your system on device boot.</p>\n<p>Another approach is to use the <code>sonic-pi-tool</code>.</p>\n<h2>Processing</h2>\n<p>Add a line <code>/usr/local/bin/processing-java --sketch=/home/pi/sketchbook/sketchname --run</code> to your startup script file.</p>\n<h1>Running a command on device connect</h1>\n<p>You may also want to run a command automatically when a device (eg a usb) is plugged in.\nOne way to do this is with udev rules.\nYou can make a new udev rule by adding a file under the <code>/etc/udev/rules.d/</code> directory.</p>\n",
      "id": 16
    },
    {
      "path": "Appendix/System_Management.md",
      "url": "Appendix/System_Management.html",
      "content": "# Assorted System Management Notes\n\n\n# Mark's highly opinionated list of things you should know/have done by the time you graduate\n\nThis list is limited to topics covered in this class. Unsurprisingly, there is a much larger set of knowledge you should have as well.\n\n- Have a unix system installed locally on a machine you own\n- Be comfortable with bash\n- You should be comfortable with either vi or emacs.\n",
      "html": "<h1>Assorted System Management Notes</h1>\n<h1>Mark’s highly opinionated list of things you should know/have done by the time you graduate</h1>\n<p>This list is limited to topics covered in this class. Unsurprisingly, there is a much larger set of knowledge you should have as well.</p>\n<ul>\n<li>Have a unix system installed locally on a machine you own</li>\n<li>Be comfortable with bash</li>\n<li>You should be comfortable with either vi or emacs.</li>\n</ul>\n",
      "id": 17
    },
    {
      "path": "Appendix/Version_Control.md",
      "url": "Appendix/Version_Control.html",
      "content": "# Version Control at Yale\n\nFor CPSC334, we will use the version control tool `git` to manage the code you write.\nYou may use one of the following platforms:\n\n- ***Github***: good for making your code publicly available. Github provides private repositories with a student account or a paid account. Also provides a nice way to make a free personal website with Github Pages.\n- ***Gitlab***: good for keeping private repositories. Gitlab provides unlimited private repositories.\n- ***Yale's Github***: also good for private repositories. Can only be accessed via the Yale network (including VPN). Unclear how long your code will exist after you graduate. Also unclear who owns any IP associated with code on Yale Github.\n\nEach platform has its own set of pros and cons - you must choose which to use based on your personal preference.\n\nAfter you have chosen a platform, please make a repository for the first module ([Generative Art](./1_Generative_Art/index.md)).\nIf your repository is public, please send us a link to each repository.\nIf your repository is private, please add both Prof. Petersen and Mark as collaborators.\nWe will use these repos to evaluate your submissions.\nSome factors that will influence your grade are frequency of commits, size of commits, and quality of commit messages.\n",
      "html": "<h1>Version Control at Yale</h1>\n<p>For CPSC334, we will use the version control tool <code>git</code> to manage the code you write.\nYou may use one of the following platforms:</p>\n<ul>\n<li><em><strong>Github</strong></em>: good for making your code publicly available. Github provides private repositories with a student account or a paid account. Also provides a nice way to make a free personal website with Github Pages.</li>\n<li><em><strong>Gitlab</strong></em>: good for keeping private repositories. Gitlab provides unlimited private repositories.</li>\n<li><em><strong>Yale’s Github</strong></em>: also good for private repositories. Can only be accessed via the Yale network (including VPN). Unclear how long your code will exist after you graduate. Also unclear who owns any IP associated with code on Yale Github.</li>\n</ul>\n<p>Each platform has its own set of pros and cons - you must choose which to use based on your personal preference.</p>\n<p>After you have chosen a platform, please make a repository for the first module (<a href=\"./1_Generative_Art/index.html\">Generative Art</a>).\nIf your repository is public, please send us a link to each repository.\nIf your repository is private, please add both Prof. Petersen and Mark as collaborators.\nWe will use these repos to evaluate your submissions.\nSome factors that will influence your grade are frequency of commits, size of commits, and quality of commit messages.</p>\n",
      "id": 18
    },
    {
      "path": "Appendix/index.md",
      "url": "Appendix/index.html",
      "content": "# Appendix\n\nYou will find materials referenced from the text here.\nOther course materials presented in class, such as lecture slides, are available on the internal Canvas course website.\n",
      "html": "<h1>Appendix</h1>\n<p>You will find materials referenced from the text here.\nOther course materials presented in class, such as lecture slides, are available on the internal Canvas course website.</p>\n",
      "id": 19
    },
    {
      "path": "ExternalResources/index.md",
      "url": "ExternalResources/index.html",
      "content": "# External Resources\n\nThis folder contains content to supplement modules\n",
      "html": "<h1>External Resources</h1>\n<p>This folder contains content to supplement modules</p>\n",
      "id": 20
    }
  ]
}